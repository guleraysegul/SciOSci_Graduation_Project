{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTSdoCfMZgTT",
        "outputId": "589a94b2-2418-4f8d-ee4f-568aaad0019e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvleUVmMZYsr",
        "outputId": "85817c71-612b-4b14-ce89-623b3ce773d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Fetching page 4...\n",
            "Fetching page 5...\n",
            "Fetching page 6...\n",
            "Fetching page 7...\n",
            "Fetching page 8...\n",
            "Fetching page 9...\n",
            "Fetching page 10...\n",
            "Scraping completed. Data saved to 'honorary_members_ba.csv' and 'honorary_members_ba.json'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Base URL for the honorary members\n",
        "base_url = \"https://bilimakademisi.org/onursal-uyeler/page/\"\n",
        "\n",
        "# Total number of pages\n",
        "total_pages = 10  # Update based on the actual number of pages\n",
        "\n",
        "# List to store member data\n",
        "honorary_members = []\n",
        "\n",
        "for page in range(1, total_pages + 1):\n",
        "    print(f\"Fetching page {page}...\")\n",
        "    url = f\"{base_url}{page}\"\n",
        "    response = requests.get(url)\n",
        "\n",
        "    if response.status_code != 200:\n",
        "        print(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
        "        continue\n",
        "\n",
        "    # Parse the page content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Locate the table containing members\n",
        "    table = soup.find(\"table\", class_=\"tablepress\")\n",
        "\n",
        "    if not table:\n",
        "        print(f\"No table found on page {page}.\")\n",
        "        continue\n",
        "\n",
        "    # Extract rows from the table\n",
        "    rows = table.find(\"tbody\").find_all(\"tr\")\n",
        "\n",
        "    for row in rows:\n",
        "        cols = row.find_all(\"td\")\n",
        "        if len(cols) < 2:\n",
        "            continue  # Skip malformed rows\n",
        "\n",
        "        # Extract data\n",
        "        member_name_and_title = cols[0].get_text(strip=True)\n",
        "        member_affiliation = cols[1].get_text(strip=True)\n",
        "        member_profile_link = cols[0].find(\"a\")[\"href\"] if cols[0].find(\"a\") else None\n",
        "\n",
        "        honorary_members.append({\n",
        "            \"name_and_title\": member_name_and_title,\n",
        "            \"affiliation\": member_affiliation,\n",
        "            \"profile_link\": member_profile_link\n",
        "        })\n",
        "\n",
        "    # Respectful scraping\n",
        "    time.sleep(1)\n",
        "\n",
        "# Save to CSV and JSON\n",
        "df = pd.DataFrame(honorary_members)\n",
        "df.to_csv(\"honorary_members_ba.csv\", index=False, encoding=\"utf-8\")\n",
        "df.to_json(\"honorary_members_ba.json\", orient=\"records\", indent=4, force_ascii=False)\n",
        "\n",
        "print(\"Scraping completed. Data saved to 'honorary_members_ba.csv' and 'honorary_members_ba.json'.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import csv\n",
        "\n",
        "# Function to extract members from a given soup object\n",
        "def extract_asli_members(soup):\n",
        "    table = soup.find(\"table\", {\"id\": \"tablepress-asliuyeler\"})\n",
        "    if not table:\n",
        "        print(\"No table found with ID 'tablepress-asliuyeler'.\")\n",
        "        return []\n",
        "\n",
        "    rows = table.find(\"tbody\").find_all(\"tr\")\n",
        "    members = []\n",
        "    for row in rows:\n",
        "        cols = row.find_all(\"td\")\n",
        "        member = {\n",
        "            \"name\": cols[0].get_text(strip=True) if len(cols) > 0 else \"\",\n",
        "            \"profile_url\": cols[0].find(\"a\")[\"href\"] if cols[0].find(\"a\") and \"href\" in cols[0].find(\"a\").attrs else \"\",\n",
        "            \"research_area\": cols[1].get_text(strip=True) if len(cols) > 1 else \"\",\n",
        "            \"university\": cols[2].get_text(strip=True) if len(cols) > 2 else \"\"\n",
        "        }\n",
        "        members.append(member)\n",
        "    return members\n",
        "\n",
        "# Function to fetch a page and parse it with BeautifulSoup\n",
        "def fetch_page(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            return BeautifulSoup(response.text, 'html.parser')\n",
        "        else:\n",
        "            print(f\"Failed to fetch page. Status code: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "# Main function to scrape data across pages\n",
        "def scrape_asli_members(base_url, pages):\n",
        "    all_members = []\n",
        "    for page in range(1, pages + 1):\n",
        "        print(f\"Fetching page {page}...\")\n",
        "        url = f\"{base_url}?paged={page}\"\n",
        "        soup = fetch_page(url)\n",
        "        if not soup:\n",
        "            break\n",
        "        members = extract_asli_members(soup)\n",
        "        if not members:\n",
        "            break\n",
        "        all_members.extend(members)\n",
        "    return all_members\n",
        "\n",
        "# Save results to JSON\n",
        "def save_to_json(data, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Save results to CSV\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=[\"name\", \"profile_url\", \"research_area\", \"university\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(data)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "# Configuration\n",
        "BASE_URL = \"https://bilimakademisi.org/uyeler/\"\n",
        "PAGES = 3  # Adjust the number of pages as needed\n",
        "\n",
        "# Run the scraper\n",
        "if __name__ == \"__main__\":\n",
        "    members = scrape_asli_members(BASE_URL, PAGES)\n",
        "    if members:\n",
        "        save_to_json(members, \"asli_members_ba.json\")\n",
        "        save_to_csv(members, \"asli_members_ba.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJbM9Wa3bBG4",
        "outputId": "55f1e0e1-20c9-41ab-d1c8-0c81724e3af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching page 1...\n",
            "Fetching page 2...\n",
            "Fetching page 3...\n",
            "Data saved to asli_members_ba.json\n",
            "Data saved to asli_members_ba.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load JSON data from a file\n",
        "def load_json(filename):\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {filename}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Count members and compare\n",
        "def count_and_compare(asli_file, honorary_file):\n",
        "    # Load members from JSON files\n",
        "    asli_members = load_json(asli_file)\n",
        "    honorary_members = load_json(honorary_file)\n",
        "\n",
        "    # Get counts\n",
        "    asli_count = len(asli_members)\n",
        "    honorary_count = len(honorary_members)\n",
        "\n",
        "    # Print counts\n",
        "    print(f\"Number of Asli Members: {asli_count}\")\n",
        "    print(f\"Number of Honorary Members: {honorary_count}\")\n",
        "\n",
        "    # Total count\n",
        "    total_count = asli_count + honorary_count\n",
        "    print(f\"Total Members: {total_count}\")\n",
        "\n",
        "# File paths\n",
        "asli_file = \"/content/asli_members_ba.json\"\n",
        "honorary_file = \"/content/honorary_members_ba.json\"\n",
        "\n",
        "# Run the comparison\n",
        "count_and_compare(asli_file, honorary_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqYUw6WMiQR-",
        "outputId": "ddfe56ac-5378-4f2c-8393-a6ac4e8e9d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Asil Members: 666\n",
            "Number of Honorary Members: 420\n",
            "Total Members: 1086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def remove_duplicates_honorary(members):\n",
        "    \"\"\"\n",
        "    Remove duplicate honorary members based on all fields.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    unique_members = []\n",
        "    for member in members:\n",
        "        member_tuple = (member[\"name_and_title\"], member[\"affiliation\"], member[\"profile_link\"])\n",
        "        if member_tuple not in seen:\n",
        "            seen.add(member_tuple)\n",
        "            unique_members.append(member)\n",
        "    return unique_members\n",
        "\n",
        "\n",
        "def remove_duplicates_asli(members):\n",
        "    \"\"\"\n",
        "    Remove duplicate asli members based on all fields.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "    unique_members = []\n",
        "    for member in members:\n",
        "        member_tuple = (member[\"name\"], member[\"profile_url\"], member[\"research_area\"], member[\"university\"])\n",
        "        if member_tuple not in seen:\n",
        "            seen.add(member_tuple)\n",
        "            unique_members.append(member)\n",
        "    return unique_members\n",
        "\n",
        "\n",
        "# Load your JSON data here\n",
        "import json\n",
        "\n",
        "# Load honorary members\n",
        "with open(\"honorary_members_ba.json\", \"r\", encoding=\"utf-8\") as honorary_file:\n",
        "    honorary_members = json.load(honorary_file)\n",
        "\n",
        "# Load asli members\n",
        "with open(\"asli_members_ba.json\", \"r\", encoding=\"utf-8\") as asli_file:\n",
        "    asli_members = json.load(asli_file)\n",
        "\n",
        "# Remove duplicates\n",
        "cleaned_honorary_members = remove_duplicates_honorary(honorary_members)\n",
        "cleaned_asli_members = remove_duplicates_asli(asli_members)\n",
        "\n",
        "print(f\"Cleaned Honorary Members: {len(cleaned_honorary_members)}\")\n",
        "print(f\"Cleaned Asli Members: {len(cleaned_asli_members)}\")\n",
        "\n",
        "# Save the cleaned lists back to JSON files\n",
        "with open(\"cleaned_honorary_members.json\", \"w\", encoding=\"utf-8\") as honorary_file:\n",
        "    json.dump(cleaned_honorary_members, honorary_file, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open(\"cleaned_asli_members.json\", \"w\", encoding=\"utf-8\") as asli_file:\n",
        "    json.dump(cleaned_asli_members, asli_file, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qUW0Qq6rbwp",
        "outputId": "4f4fd766-0505-4ccd-f6e1-f1d7e6b4b55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Honorary Members: 42\n",
            "Cleaned Asli Members: 222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Load cleaned Asli and Honorary members JSON files\n",
        "with open(\"cleaned_asli_members.json\", \"r\", encoding=\"utf-8\") as f_asli:\n",
        "    asli_members = json.load(f_asli)\n",
        "\n",
        "with open(\"cleaned_honorary_members.json\", \"r\", encoding=\"utf-8\") as f_honorary:\n",
        "    honorary_members = json.load(f_honorary)\n",
        "\n",
        "# Combine both lists\n",
        "all_members = asli_members + honorary_members\n",
        "\n",
        "# Check for duplicates\n",
        "seen = set()\n",
        "duplicates = []\n",
        "unique_members = []\n",
        "for member in all_members:\n",
        "    # Use 'name' or 'name_and_title' fields for comparison\n",
        "    name = member.get(\"name\", \"\").strip().lower() or member.get(\"name_and_title\", \"\").strip().lower()\n",
        "    if name in seen:\n",
        "        duplicates.append(member)  # Add the full duplicate entry for review\n",
        "    else:\n",
        "        seen.add(name)\n",
        "        unique_members.append(member)  # Add only unique members to the final list\n",
        "\n",
        "# Print duplicates for manual review\n",
        "if duplicates:\n",
        "    print(\"Duplicate Entries Found:\")\n",
        "    for dup in duplicates:\n",
        "        print(dup)\n",
        "else:\n",
        "    print(\"No duplicate entries found.\")\n",
        "\n",
        "# Save all unique members to a new JSON file\n",
        "with open(\"all_members_ba.json\", \"w\", encoding=\"utf-8\") as f_all_members:\n",
        "    json.dump(unique_members, f_all_members, indent=4, ensure_ascii=False)\n",
        "\n",
        "print(\"Combined members saved to 'all_members_ba.json'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KA1-DjOetzhn",
        "outputId": "3f8a646b-9fd4-48c3-a2d3-75146fd9633f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No duplicate entries found.\n",
            "Combined members saved to 'all_members_ba.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Names to check (deceased members)\n",
        "deceased_names = [\n",
        "    \"Miral Dizdaroğlu\",\n",
        "    \"Fuat Keyman\",\n",
        "    \"Durmuş Ali Demir\",\n",
        "    \"Zafer Toprak\",\n",
        "    \"Ayhan Ulubelen\",\n",
        "    \"Hamit Fişek\",\n",
        "    \"Philip W. Anderson\",\n",
        "    \"Rahmi Güven\",\n",
        "    \"David Pines\",\n",
        "    \"Güven Arsebük\",\n",
        "    \"Çiğdem Kağıtçıbaşı\",\n",
        "    \"Yücel Kanpolat\",\n",
        "    \"Tosun Terzioğlu\",\n",
        "    \"Namık Kemal Pak\"\n",
        "]\n",
        "\n",
        "# Normalize the names in deceased list\n",
        "deceased_names_normalized = [name.strip().lower() for name in deceased_names]\n",
        "\n",
        "# Load the all_members.json file\n",
        "with open(\"all_members_ba.json\", \"r\", encoding=\"utf-8\") as f_all_members:\n",
        "    all_members = json.load(f_all_members)\n",
        "\n",
        "# Extract and normalize names from all_members\n",
        "all_member_names = set(\n",
        "    member.get(\"name\", \"\").strip().lower() for member in all_members if \"name\" in member\n",
        ") | set(\n",
        "    member.get(\"name_and_title\", \"\").split(\" -\")[0].strip().lower()\n",
        "    for member in all_members if \"name_and_title\" in member\n",
        ")\n",
        "\n",
        "# Check for matches\n",
        "matches = [name for name in deceased_names_normalized if name in all_member_names]\n",
        "\n",
        "# Print results\n",
        "if matches:\n",
        "    print(\"The following deceased members are already in all_members_ba.json:\")\n",
        "    for match in matches:\n",
        "        print(match)\n",
        "else:\n",
        "    print(\"No matches found. These deceased members are not in all_members_ba.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdQiX-Wczi3g",
        "outputId": "8ec775a9-0c80-4e0d-91ea-52eb8a4b2f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following deceased members are already in all_members_ba.json:\n",
            "david pines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file_path = \"/content/all_members_ba.json\"  # Replace with actual file path\n",
        "output_file_path = \"ba_author_ids.json\"\n",
        "\n",
        "# Load all members JSON\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    all_members = json.load(file)\n",
        "\n",
        "# OpenAlex API endpoint\n",
        "openalex_base_url = \"https://api.openalex.org/authors\"\n",
        "\n",
        "# Function to query OpenAlex\n",
        "def get_openalex_author_id(name):\n",
        "    try:\n",
        "        response = requests.get(openalex_base_url, params={\"search\": name})\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        results = response.json()\n",
        "\n",
        "        if \"results\" in results and results[\"results\"]:\n",
        "            # Take the first match (or apply more filters if needed)\n",
        "            first_result = results[\"results\"][0]\n",
        "            return {\n",
        "                \"author_id\": first_result.get(\"id\"),\n",
        "                \"name\": first_result.get(\"display_name\"),\n",
        "                \"works_count\": first_result.get(\"works_count\", 0),\n",
        "                \"cited_by_count\": first_result.get(\"cited_by_count\", 0),\n",
        "            }\n",
        "        else:\n",
        "            return None  # No match found\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching author ID for {name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process each member to fetch OpenAlex Author ID\n",
        "results = []\n",
        "for member in all_members:\n",
        "    # Get name from \"name\" or \"name_and_title\"\n",
        "    name = member.get(\"name\") or member.get(\"name_and_title\")\n",
        "    print(f\"Fetching OpenAlex Author ID for {name}...\")\n",
        "\n",
        "    # Fetch author data from OpenAlex\n",
        "    author_data = get_openalex_author_id(name)\n",
        "\n",
        "    # Construct the result entry\n",
        "    result_entry = {\n",
        "        \"name\": member.get(\"name\"),\n",
        "        \"affiliation\": member.get(\"affiliation\"),\n",
        "        \"profile_link\": member.get(\"profile_link\") or member.get(\"profile_url\"),\n",
        "        \"research_area\": member.get(\"research_area\"),\n",
        "        \"university\": member.get(\"university\"),\n",
        "        \"openalex_id\": author_data[\"author_id\"] if author_data else None,\n",
        "        \"works_count\": author_data[\"works_count\"] if author_data else None,\n",
        "        \"cited_by_count\": author_data[\"cited_by_count\"] if author_data else None,\n",
        "    }\n",
        "\n",
        "    results.append(result_entry)\n",
        "    # Respect API rate limits\n",
        "    time.sleep(1)  # Adjust delay if needed\n",
        "\n",
        "# Save the results to a JSON file\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(results, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Author IDs saved to '{output_file_path}'.\")\n",
        "\n",
        "# Optionally, save to CSV for easier inspection\n",
        "pd.DataFrame(results).to_csv(\"ba_author_ids.csv\", index=False)\n",
        "print(\"Author IDs also saved to 'ba_author_ids.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5-bnhVA4cqy",
        "outputId": "93a62c6a-c8f2-4c4b-f831-ce7488ec360a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching OpenAlex Author ID for Can Fuat Delale...\n",
            "Fetching OpenAlex Author ID for Mehmet Özdoğan...\n",
            "Fetching OpenAlex Author ID for Aslıhan Yener...\n",
            "Fetching OpenAlex Author ID for M. Ali Alpar...\n",
            "Fetching OpenAlex Author ID for Ersin Göğüş...\n",
            "Fetching OpenAlex Author ID for Feryal Özel...\n",
            "Fetching OpenAlex Author ID for Ethem Alpaydın...\n",
            "Fetching OpenAlex Author ID for Lale Akarun...\n",
            "Fetching OpenAlex Author ID for Tamer Özsu...\n",
            "Fetching OpenAlex Author ID for İzak Benbasat...\n",
            "Fetching OpenAlex Author ID for Attila Gürsoy...\n",
            "Fetching OpenAlex Author ID for Pekcan Ungan...\n",
            "Fetching OpenAlex Author ID for Canan Atılgan...\n",
            "Fetching OpenAlex Author ID for İvet Bahar...\n",
            "Fetching OpenAlex Author ID for Mustafa Tekin...\n",
            "Fetching OpenAlex Author ID for Şermin Genç...\n",
            "Fetching OpenAlex Author ID for Mehmet Öztürk...\n",
            "Fetching OpenAlex Author ID for Vasıf Hasırcı...\n",
            "Fetching OpenAlex Author ID for Berrin Tansel...\n",
            "Fetching OpenAlex Author ID for Nilsun İnce...\n",
            "Fetching OpenAlex Author ID for Orhan Yenigün...\n",
            "Fetching OpenAlex Author ID for Delia Sponza...\n",
            "Fetching OpenAlex Author ID for Mehmet Kobya...\n",
            "Fetching OpenAlex Author ID for İdil Arslan Alaton...\n",
            "Fetching OpenAlex Author ID for Derin Orhon...\n",
            "Fetching OpenAlex Author ID for İhsan Çalış...\n",
            "Fetching OpenAlex Author ID for Hüsnü Can Başer...\n",
            "Fetching OpenAlex Author ID for Sedat Ölçer...\n",
            "Fetching OpenAlex Author ID for Tolga Mete Duman...\n",
            "Fetching OpenAlex Author ID for Enis Çetin...\n",
            "Fetching OpenAlex Author ID for Ergin Atalar...\n",
            "Fetching OpenAlex Author ID for Bülent Sankur...\n",
            "Fetching OpenAlex Author ID for Erol Gelenbe...\n",
            "Fetching OpenAlex Author ID for Erdal Panayırcı...\n",
            "Fetching OpenAlex Author ID for Özgür Barış Akan...\n",
            "Fetching OpenAlex Author ID for Alper Demir...\n",
            "Fetching OpenAlex Author ID for Murat Tekalp...\n",
            "Fetching OpenAlex Author ID for Hakan Ürey...\n",
            "Fetching OpenAlex Author ID for Asuman Özdağlar...\n",
            "Fetching OpenAlex Author ID for Elza Erkip...\n",
            "Fetching OpenAlex Author ID for Haluk Külah...\n",
            "Fetching OpenAlex Author ID for Aydın Alatan...\n",
            "Fetching OpenAlex Author ID for Murat Uysal...\n",
            "Fetching OpenAlex Author ID for Aylin Yener...\n",
            "Fetching OpenAlex Author ID for Selim Ünlü...\n",
            "Fetching OpenAlex Author ID for Fikri Karaesmen...\n",
            "Fetching OpenAlex Author ID for Murat Köksalan...\n",
            "Fetching OpenAlex Author ID for Pınar Keskinocak...\n",
            "Fetching OpenAlex Author ID for Sibel Salman...\n",
            "Fetching OpenAlex Author ID for Gürol Irzık...\n",
            "Fetching OpenAlex Author ID for Alpan Bek...\n",
            "Fetching OpenAlex Author ID for Yalçın Elerman...\n",
            "Fetching OpenAlex Author ID for Ercan Alp...\n",
            "Fetching OpenAlex Author ID for Emrah Kalemci...\n",
            "Fetching OpenAlex Author ID for Ömer İlday...\n",
            "Fetching OpenAlex Author ID for Metin Gürses...\n",
            "Fetching OpenAlex Author ID for Bilal Tanatar...\n",
            "Fetching OpenAlex Author ID for Oğuz Gülseren...\n",
            "Fetching OpenAlex Author ID for Metin Arık...\n",
            "Fetching OpenAlex Author ID for Özhan Özatay...\n",
            "Fetching OpenAlex Author ID for Mehmet Erbudak...\n",
            "Fetching OpenAlex Author ID for Uğur Tırnaklı...\n",
            "Fetching OpenAlex Author ID for Ayşe Erzan...\n",
            "Fetching OpenAlex Author ID for İsmail Hakkı Duru...\n",
            "Fetching OpenAlex Author ID for Ramazan Tuğrul Senger...\n",
            "Fetching OpenAlex Author ID for Nejat Bulut...\n",
            "Fetching OpenAlex Author ID for Önder Pekcan...\n",
            "Fetching OpenAlex Author ID for Ali Mostafazadeh...\n",
            "Fetching OpenAlex Author ID for Alphan Sennaroğlu...\n",
            "Fetching OpenAlex Author ID for Özgür Müstecaplıoğlu...\n",
            "Fetching OpenAlex Author ID for Tekin Dereli...\n",
            "Fetching OpenAlex Author ID for Mahmut Hortaçsu...\n",
            "Fetching OpenAlex Author ID for Serdar Öğüt...\n",
            "Fetching OpenAlex Author ID for Altan Baykal...\n",
            "Fetching OpenAlex Author ID for Ahmet Oral...\n",
            "Fetching OpenAlex Author ID for Altuğ Özpineci...\n",
            "Fetching OpenAlex Author ID for Mehmet Tomak...\n",
            "Fetching OpenAlex Author ID for İnanç Adagideli...\n",
            "Fetching OpenAlex Author ID for Cihan Saçlıoğlu...\n",
            "Fetching OpenAlex Author ID for Şinasi Ellialtıoğlu...\n",
            "Fetching OpenAlex Author ID for Ergin Sezgin...\n",
            "Fetching OpenAlex Author ID for Mete Atatüre...\n",
            "Fetching OpenAlex Author ID for Baha Balantekin...\n",
            "Fetching OpenAlex Author ID for Refik Kortan...\n",
            "Fetching OpenAlex Author ID for Umut Gürsoy...\n",
            "Fetching OpenAlex Author ID for Umran İnan...\n",
            "Fetching OpenAlex Author ID for Tahir Çağın...\n",
            "Fetching OpenAlex Author ID for Türkan Haliloğlu...\n",
            "Fetching OpenAlex Author ID for Özlem Keskin...\n",
            "Fetching OpenAlex Author ID for Ünal Tekinalp...\n",
            "Fetching OpenAlex Author ID for Bertil Emrah Oder...\n",
            "Fetching OpenAlex Author ID for Yeşim Atamer...\n",
            "Fetching OpenAlex Author ID for Refet Gürkaynak...\n",
            "Fetching OpenAlex Author ID for Ayşe Buğra Kavala...\n",
            "Fetching OpenAlex Author ID for Tayfun Sönmez...\n",
            "Fetching OpenAlex Author ID for M. Utku Ünver...\n",
            "Fetching OpenAlex Author ID for Dani Rodrik...\n",
            "Fetching OpenAlex Author ID for Erinç Yeldan...\n",
            "Fetching OpenAlex Author ID for Kamil Yılmaz...\n",
            "Fetching OpenAlex Author ID for Daron Acemoğlu...\n",
            "Fetching OpenAlex Author ID for Erol Taymaz...\n",
            "Fetching OpenAlex Author ID for Ahmet Alkan...\n",
            "Fetching OpenAlex Author ID for Nezih Güner...\n",
            "Fetching OpenAlex Author ID for Ufuk Akçiğit...\n",
            "Fetching OpenAlex Author ID for Ayşe İmrohoroğlu...\n",
            "Fetching OpenAlex Author ID for Şevket Pamuk...\n",
            "Fetching OpenAlex Author ID for Hakkı Polat Gülkan...\n",
            "Fetching OpenAlex Author ID for Bülent Mengüç...\n",
            "Fetching OpenAlex Author ID for Cahit Helvacı...\n",
            "Fetching OpenAlex Author ID for Attila Çiner...\n",
            "Fetching OpenAlex Author ID for Namık Çağatay...\n",
            "Fetching OpenAlex Author ID for Erdin Bozkurt...\n",
            "Fetching OpenAlex Author ID for Ali Polat...\n",
            "Fetching OpenAlex Author ID for Okan Tüysüz...\n",
            "Fetching OpenAlex Author ID for Ziyadin Çakır...\n",
            "Fetching OpenAlex Author ID for Seda Keskin...\n",
            "Fetching OpenAlex Author ID for Şefik Süzer...\n",
            "Fetching OpenAlex Author ID for Ömer Dağ...\n",
            "Fetching OpenAlex Author ID for Viktorya Aviyente...\n",
            "Fetching OpenAlex Author ID for Engin Umut Akkaya...\n",
            "Fetching OpenAlex Author ID for Olgun Güven...\n",
            "Fetching OpenAlex Author ID for Adil Denizli...\n",
            "Fetching OpenAlex Author ID for Ersin Yurtsever...\n",
            "Fetching OpenAlex Author ID for Can Erkey...\n",
            "Fetching OpenAlex Author ID for Nesrin Hasırcı...\n",
            "Fetching OpenAlex Author ID for Saim Özkar...\n",
            "Fetching OpenAlex Author ID for Cihangir Tanyeli...\n",
            "Fetching OpenAlex Author ID for Levent Toppare...\n",
            "Fetching OpenAlex Author ID for İlhan Aksay...\n",
            "Fetching OpenAlex Author ID for Çağatay Başdoğan...\n",
            "Fetching OpenAlex Author ID for Nevzat Özgüven...\n",
            "Fetching OpenAlex Author ID for M. Pınar Mengüç...\n",
            "Fetching OpenAlex Author ID for Ali Erdemir...\n",
            "Fetching OpenAlex Author ID for Ergün Yalçın...\n",
            "Fetching OpenAlex Author ID for Cem Yalçın Yıldırım...\n",
            "Fetching OpenAlex Author ID for Ali Nesin...\n",
            "Fetching OpenAlex Author ID for Ali Ülger...\n",
            "Fetching OpenAlex Author ID for Hüsnü Ata Erbay...\n",
            "Fetching OpenAlex Author ID for Halil Mete Soner...\n",
            "Fetching OpenAlex Author ID for Hilmi Demiray...\n",
            "Fetching OpenAlex Author ID for İlhan Tekeli...\n",
            "Fetching OpenAlex Author ID for Zeynep Çelik...\n",
            "Fetching OpenAlex Author ID for İhsan Gürsel...\n",
            "Fetching OpenAlex Author ID for Tamer Önder...\n",
            "Fetching OpenAlex Author ID for Aydoğan Özcan...\n",
            "Fetching OpenAlex Author ID for Metin Sitti...\n",
            "Fetching OpenAlex Author ID for Kamil Uğurbil...\n",
            "Fetching OpenAlex Author ID for Levent Demirel...\n",
            "Fetching OpenAlex Author ID for Ayhan Aksu Koç...\n",
            "Fetching OpenAlex Author ID for Ayşe Nuray Karancı...\n",
            "Fetching OpenAlex Author ID for Gün Şemin...\n",
            "Fetching OpenAlex Author ID for Zeynep Aycan...\n",
            "Fetching OpenAlex Author ID for Mehmet Eskin...\n",
            "Fetching OpenAlex Author ID for Sami Gülgöz...\n",
            "Fetching OpenAlex Author ID for Nebi Sümer...\n",
            "Fetching OpenAlex Author ID for Ayhan Kaya...\n",
            "Fetching OpenAlex Author ID for Yeşim Arat...\n",
            "Fetching OpenAlex Author ID for Ali Çarkoğlu...\n",
            "Fetching OpenAlex Author ID for Ahmet İçduygu...\n",
            "Fetching OpenAlex Author ID for Ersin Kalaycıoğlu...\n",
            "Fetching OpenAlex Author ID for Yılmaz Esmer...\n",
            "Fetching OpenAlex Author ID for Çağlar Keyder...\n",
            "Fetching OpenAlex Author ID for Ayşe Öncü...\n",
            "Fetching OpenAlex Author ID for Yasemin Soysal...\n",
            "Fetching OpenAlex Author ID for Deniz Kandiyoti...\n",
            "Fetching OpenAlex Author ID for Edhem Eldem...\n",
            "Fetching OpenAlex Author ID for İsenbike Togan...\n",
            "Fetching OpenAlex Author ID for Reşat Kasaba...\n",
            "Fetching OpenAlex Author ID for Moshe Arditi...\n",
            "Fetching OpenAlex Author ID for Okan Bülent Yıldız...\n",
            "Fetching OpenAlex Author ID for Murat Akova...\n",
            "Fetching OpenAlex Author ID for Önder Ergönül...\n",
            "Fetching OpenAlex Author ID for İsmail Hakkı Ulus...\n",
            "Fetching OpenAlex Author ID for Mustafa İlhan...\n",
            "Fetching OpenAlex Author ID for Hakan Orer...\n",
            "Fetching OpenAlex Author ID for Kemal Sıtkı Türker...\n",
            "Fetching OpenAlex Author ID for Berrak Çağlayan Yeğen...\n",
            "Fetching OpenAlex Author ID for Ramazan İdilman...\n",
            "Fetching OpenAlex Author ID for Feza Remzi...\n",
            "Fetching OpenAlex Author ID for Murat Karaçorlu...\n",
            "Fetching OpenAlex Author ID for Meral Beksaç...\n",
            "Fetching OpenAlex Author ID for Reyhan Küçükkaya...\n",
            "Fetching OpenAlex Author ID for Lale Tokgözoğlu...\n",
            "Fetching OpenAlex Author ID for Mustafa Arıcı...\n",
            "Fetching OpenAlex Author ID for Deniz Kırık...\n",
            "Fetching OpenAlex Author ID for Cumhur Ertekin...\n",
            "Fetching OpenAlex Author ID for Turgay Dalkara...\n",
            "Fetching OpenAlex Author ID for Aksel Siva...\n",
            "Fetching OpenAlex Author ID for Marsel Meşulam...\n",
            "Fetching OpenAlex Author ID for Necmettin Pamir...\n",
            "Fetching OpenAlex Author ID for Şevket Ruacan...\n",
            "Fetching OpenAlex Author ID for Tarık Tihan...\n",
            "Fetching OpenAlex Author ID for Seza Özen...\n",
            "Fetching OpenAlex Author ID for Okan Akhan...\n",
            "Fetching OpenAlex Author ID for Ahmet Gül...\n",
            "Fetching OpenAlex Author ID for Hasan Yazıcı...\n",
            "Fetching OpenAlex Author ID for Hülya Kayserili...\n",
            "Fetching OpenAlex Author ID for Alp Can...\n",
            "Fetching OpenAlex Author ID for Sevtap Arıkan Akdağlı...\n",
            "Fetching OpenAlex Author ID for Haluk Özen...\n",
            "Fetching OpenAlex Author ID for Mehmet Toner...\n",
            "Fetching OpenAlex Author ID for Naci Görür...\n",
            "Fetching OpenAlex Author ID for Yücel Yılmaz...\n",
            "Fetching OpenAlex Author ID for İsmail Çakmak...\n",
            "Fetching OpenAlex Author ID for Cem Ersoy...\n",
            "Fetching OpenAlex Author ID for Seda Ertaç...\n",
            "Fetching OpenAlex Author ID for Ece Göztepe...\n",
            "Fetching OpenAlex Author ID for Ali Hortaçsu...\n",
            "Fetching OpenAlex Author ID for Yasemin Gürsoy Özdemir...\n",
            "Fetching OpenAlex Author ID for Kübra Doğan Yenisey...\n",
            "Fetching OpenAlex Author ID for Ali İzzet Tekcan...\n",
            "Fetching OpenAlex Author ID for Canan Sümer...\n",
            "Fetching OpenAlex Author ID for Serpil Sayın...\n",
            "Fetching OpenAlex Author ID for Metin Muradoğlu...\n",
            "Fetching OpenAlex Author ID for Murat Mungan...\n",
            "Fetching OpenAlex Author ID for Aylin Küntay...\n",
            "Fetching OpenAlex Author ID for Semih Koray...\n",
            "Fetching OpenAlex Author ID for Alev Devrim Güçlü...\n",
            "Fetching OpenAlex Author ID for Nesim Erkip...\n",
            "Fetching OpenAlex Author ID for Başak Çalı...\n",
            "Fetching OpenAlex Author ID for Burcu Balçık...\n",
            "Fetching OpenAlex Author ID for Orhan Arıkan...\n",
            "Fetching OpenAlex Author ID for Robert P. Langlands...\n",
            "Fetching OpenAlex Author ID for Edward Witten...\n",
            "Fetching OpenAlex Author ID for Hakkı Erdal Akalın...\n",
            "Fetching OpenAlex Author ID for Takhmasib Aliev...\n",
            "Fetching OpenAlex Author ID for Orhan Altan...\n",
            "Fetching OpenAlex Author ID for Yoshinori Asakawa...\n",
            "Fetching OpenAlex Author ID for Itzhak Bars...\n",
            "Fetching OpenAlex Author ID for Dame Jocelyn Bell Burnell...\n",
            "Fetching OpenAlex Author ID for Şeyla Benhabib...\n",
            "Fetching OpenAlex Author ID for Edouard Brezin...\n",
            "Fetching OpenAlex Author ID for Martin Chalfie...\n",
            "Fetching OpenAlex Author ID for Ayşe Çağlar...\n",
            "Fetching OpenAlex Author ID for Peter A. Diamond...\n",
            "Fetching OpenAlex Author ID for Üstün Ergüder...\n",
            "Fetching OpenAlex Author ID for Suraiya Faroqhi...\n",
            "Fetching OpenAlex Author ID for Dagfinn Follesdal...\n",
            "Fetching OpenAlex Author ID for Teo Grünberg...\n",
            "Fetching OpenAlex Author ID for Dimitri Gutas...\n",
            "Fetching OpenAlex Author ID for Ayla Zırh Gürsoy...\n",
            "Fetching OpenAlex Author ID for Avedis Hacınlıyan...\n",
            "Fetching OpenAlex Author ID for Philip Kitcher...\n",
            "Fetching OpenAlex Author ID for İoanna Kuçuradi...\n",
            "Fetching OpenAlex Author ID for Timur Kuran...\n",
            "Fetching OpenAlex Author ID for Joel Lebowitz...\n",
            "Fetching OpenAlex Author ID for Dan P. McKenzie...\n",
            "Fetching OpenAlex Author ID for Sanjit K. Mitra...\n",
            "Fetching OpenAlex Author ID for Cumhur Öner...\n",
            "Fetching OpenAlex Author ID for John Polanyi...\n",
            "Fetching OpenAlex Author ID for Atta-ur Rahman...\n",
            "Fetching OpenAlex Author ID for Lord Martin Rees...\n",
            "Fetching OpenAlex Author ID for Sir Adam Roberts...\n",
            "Fetching OpenAlex Author ID for Hikmet Sarı...\n",
            "Fetching OpenAlex Author ID for İskender Sayek...\n",
            "Fetching OpenAlex Author ID for Zehra Sayers...\n",
            "Fetching OpenAlex Author ID for İzzettin Silier...\n",
            "Fetching OpenAlex Author ID for Christopher Llewellyn Smith...\n",
            "Fetching OpenAlex Author ID for Günter Stock...\n",
            "Fetching OpenAlex Author ID for Alessandro Vespignani...\n",
            "Fetching OpenAlex Author ID for Dame Hellen Wallace...\n",
            "Fetching OpenAlex Author ID for David Pines...\n",
            "Fetching OpenAlex Author ID for Philip W. Anderson...\n",
            "Fetching OpenAlex Author ID for Erdal Yıldırım...\n",
            "Fetching OpenAlex Author ID for Miral Dizdaroğlu...\n",
            "Fetching OpenAlex Author ID for Fuat Keyman...\n",
            "Fetching OpenAlex Author ID for Durmuş Ali Demir...\n",
            "Fetching OpenAlex Author ID for Zafer Toprak...\n",
            "Fetching OpenAlex Author ID for Ayhan Ulubelen...\n",
            "Fetching OpenAlex Author ID for Hamit Fişek...\n",
            "Fetching OpenAlex Author ID for Rahmi Güven...\n",
            "Fetching OpenAlex Author ID for Güven Arsebük...\n",
            "Fetching OpenAlex Author ID for Çiğdem Kağıtçıbaşı...\n",
            "Fetching OpenAlex Author ID for Yücel Kanpolat...\n",
            "Fetching OpenAlex Author ID for Tosun Terzioğlu...\n",
            "Fetching OpenAlex Author ID for Namık Kemal Pak...\n",
            "Author IDs saved to 'ba_author_ids.json'.\n",
            "Author IDs also saved to 'ba_author_ids.csv'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/ba_author_ids.json\"  # Replace with your actual file path\n",
        "output_file = \"ba_members_with_indices.json\"  # File to save the updated data\n",
        "\n",
        "# OpenAlex base API URL\n",
        "openalex_base_url = \"https://api.openalex.org\"\n",
        "\n",
        "# Function to fetch H-index and I10-index for an author\n",
        "def fetch_author_metrics(author_id):\n",
        "    url = f\"{openalex_base_url}/authors/{author_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        h_index = data.get(\"summary_stats\", {}).get(\"h_index\", None)\n",
        "        i10_index = data.get(\"summary_stats\", {}).get(\"i10_index\", None)\n",
        "        return h_index, i10_index\n",
        "    else:\n",
        "        print(f\"Error fetching data for author {author_id}: {response.status_code}\")\n",
        "        return None, None\n",
        "\n",
        "# Load the existing JSON file\n",
        "with open(input_file, \"r\") as infile:\n",
        "    members = json.load(infile)\n",
        "\n",
        "# Process each member and fetch H-index and I10-index\n",
        "for member in members:\n",
        "    openalex_ids = member.get(\"openalex_id\")\n",
        "    if isinstance(openalex_ids, list):\n",
        "        # Initialize variables to aggregate metrics\n",
        "        h_indices = []\n",
        "        i10_indices = []\n",
        "        for openalex_id in openalex_ids:\n",
        "            author_id = openalex_id.split(\"/\")[-1]  # Extract author ID\n",
        "            h_index, i10_index = fetch_author_metrics(author_id)\n",
        "            if h_index is not None:\n",
        "                h_indices.append(h_index)\n",
        "            if i10_index is not None:\n",
        "                i10_indices.append(i10_index)\n",
        "            time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "        # Aggregate indices (e.g., take maximum values)\n",
        "        member[\"h_index\"] = max(h_indices) if h_indices else None\n",
        "        member[\"i10_index\"] = max(i10_indices) if i10_indices else None\n",
        "\n",
        "    elif isinstance(openalex_ids, str):\n",
        "        author_id = openalex_ids.split(\"/\")[-1]  # Extract author ID\n",
        "        h_index, i10_index = fetch_author_metrics(author_id)\n",
        "        member[\"h_index\"] = h_index\n",
        "        member[\"i10_index\"] = i10_index\n",
        "        time.sleep(1)  # Respect API rate limits\n",
        "    else:\n",
        "        print(f\"No OpenAlex ID for {member['name']}. Skipping...\")\n",
        "        member[\"h_index\"] = None\n",
        "        member[\"i10_index\"] = None\n",
        "\n",
        "# Save the updated JSON with indices\n",
        "with open(output_file, \"w\") as outfile:\n",
        "    json.dump(members, outfile, indent=4)\n",
        "\n",
        "print(f\"H-index and I10-index added and saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0lJnR-bApwy",
        "outputId": "51d4dfbf-2358-4ba3-8e82-04b463614430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No OpenAlex ID for Ayşe Buğra Kavala. Skipping...\n",
            "No OpenAlex ID for Hakkı Erdal Akalın. Skipping...\n",
            "No OpenAlex ID for Dame Jocelyn Bell Burnell. Skipping...\n",
            "No OpenAlex ID for Ayla Zırh Gürsoy. Skipping...\n",
            "No OpenAlex ID for Avedis Hacınlıyan. Skipping...\n",
            "No OpenAlex ID for Lord Martin Rees. Skipping...\n",
            "No OpenAlex ID for İzzettin Silier. Skipping...\n",
            "No OpenAlex ID for Dame Hellen Wallace. Skipping...\n",
            "H-index and I10-index added and saved to ba_members_with_indices.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"ba_members_with_indices.json\"  # Replace with your distorted JSON file path\n",
        "output_file = \"ba_members_with_indices_restored.json\"  # Restored output file\n",
        "\n",
        "# Load the distorted JSON file\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "# Decode Unicode escape sequences\n",
        "def decode_unicode(data):\n",
        "    if isinstance(data, dict):\n",
        "        return {key: decode_unicode(value) for key, value in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [decode_unicode(item) for item in data]\n",
        "    elif isinstance(data, str):\n",
        "        try:\n",
        "            # Attempt to decode the string\n",
        "            return data.encode('latin1').decode('utf-8')\n",
        "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "            # Return the original string if decoding fails\n",
        "            return data\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "restored_data = decode_unicode(data)\n",
        "\n",
        "# Save the restored JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    json.dump(restored_data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Restored file saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcxJt62wHywc",
        "outputId": "363408f6-c48a-40b9-9a0e-5b75a435a3e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored file saved to ba_members_with_indices_restored.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/ba_members_with_indices_restored.json\"  # Replace with your actual file path\n",
        "output_json_file = \"ba_works_filtered.json\"  # JSON file output\n",
        "output_csv_file = \"ba_works_filtered.csv\"  # CSV file output\n",
        "progress_log_file = \"progress_log3.json\"  # Log file to track progress\n",
        "\n",
        "# OpenAlex base API URL\n",
        "openalex_base_url = \"https://api.openalex.org\"\n",
        "\n",
        "# Function to fetch works for an author\n",
        "def fetch_author_works(author_id):\n",
        "    works = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        url = f\"{openalex_base_url}/works?filter=author.id:{author_id}&per-page=200&page={page}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            for work in data.get(\"results\", []):  # Handle missing 'results'\n",
        "                filtered_work = {\n",
        "                    \"id\": work.get(\"id\", \"\"),\n",
        "                    \"title\": work.get(\"title\", \"\"),\n",
        "                    \"doi\": work.get(\"doi\", \"\"),\n",
        "                    \"publication_year\": work.get(\"publication_year\", \"\"),\n",
        "                    \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
        "                    \"authorships\": [\n",
        "                        {\n",
        "                            \"author\": {\n",
        "                                \"id\": auth.get(\"author\", {}).get(\"id\", \"\"),\n",
        "                                \"display_name\": auth.get(\"author\", {}).get(\"display_name\", \"\")\n",
        "                            },\n",
        "                            \"institutions\": [\n",
        "                                {\n",
        "                                    \"id\": inst.get(\"id\", \"\"),\n",
        "                                    \"display_name\": inst.get(\"display_name\", \"\"),\n",
        "                                    \"country_code\": inst.get(\"country_code\", \"\"),\n",
        "                                    \"type\": inst.get(\"type\", \"\")  # Safely handle missing type\n",
        "                                }\n",
        "                                for inst in auth.get(\"institutions\", []) if inst\n",
        "                            ]\n",
        "                        }\n",
        "                        for auth in work.get(\"authorships\", []) if auth\n",
        "                    ],\n",
        "                    \"primary_topic\": {\n",
        "                        \"id\": work.get(\"primary_topic\", {}).get(\"id\", \"\"),\n",
        "                        \"display_name\": work.get(\"primary_topic\", {}).get(\"display_name\", \"\"),\n",
        "                        \"score\": work.get(\"primary_topic\", {}).get(\"score\", 0),\n",
        "                        \"field\": {\n",
        "                            \"id\": work.get(\"primary_topic\", {}).get(\"field\", {}).get(\"id\", \"\"),\n",
        "                            \"display_name\": work.get(\"primary_topic\", {}).get(\"field\", {}).get(\"display_name\", \"\")\n",
        "                        } if work.get(\"primary_topic\", {}).get(\"field\") else {},\n",
        "                        \"subfield\": {\n",
        "                            \"id\": work.get(\"primary_topic\", {}).get(\"subfield\", {}).get(\"id\", \"\"),\n",
        "                            \"display_name\": work.get(\"primary_topic\", {}).get(\"subfield\", {}).get(\"display_name\", \"\")\n",
        "                        } if work.get(\"primary_topic\", {}).get(\"subfield\") else {},\n",
        "                        \"domain\": {\n",
        "                            \"id\": work.get(\"primary_topic\", {}).get(\"domain\", {}).get(\"id\", \"\"),\n",
        "                            \"display_name\": work.get(\"primary_topic\", {}).get(\"domain\", {}).get(\"display_name\", \"\")\n",
        "                        } if work.get(\"primary_topic\", {}).get(\"domain\") else {}\n",
        "                    } if work.get(\"primary_topic\") else {},  # Handle missing primary_topic\n",
        "                    \"concepts\": [\n",
        "                        {\n",
        "                            \"id\": concept.get(\"id\", \"\"),\n",
        "                            \"display_name\": concept.get(\"display_name\", \"\"),\n",
        "                            \"level\": concept.get(\"level\", 0),\n",
        "                            \"score\": concept.get(\"score\", 0)\n",
        "                        }\n",
        "                        for concept in work.get(\"concepts\", []) if concept\n",
        "                    ],\n",
        "                    \"open_access\": work.get(\"open_access\", {}),\n",
        "                    \"sustainable_development_goals\": [\n",
        "                        {\n",
        "                            \"id\": sdg.get(\"id\", \"\"),\n",
        "                            \"score\": sdg.get(\"score\", 0),\n",
        "                            \"display_name\": sdg.get(\"display_name\", \"\")\n",
        "                        }\n",
        "                        for sdg in work.get(\"sustainable_development_goals\", []) if sdg\n",
        "                    ],\n",
        "                    \"referenced_works\": work.get(\"referenced_works\", [])\n",
        "                }\n",
        "                works.append(filtered_work)\n",
        "            if \"next\" not in data.get(\"meta\", {}):  # Handle missing 'meta'\n",
        "                break\n",
        "            page += 1\n",
        "            time.sleep(1)  # Respect API rate limits\n",
        "        else:\n",
        "            print(f\"Error fetching works for author {author_id}: {response.status_code}\")\n",
        "            break\n",
        "    return works\n",
        "\n",
        "# Load the existing JSON file\n",
        "with open(input_file, \"r\") as infile:\n",
        "    members = json.load(infile)\n",
        "\n",
        "# Load or initialize the progress log\n",
        "try:\n",
        "    with open(progress_log_file, \"r\") as log_file:\n",
        "        progress_log = json.load(log_file)\n",
        "except FileNotFoundError:\n",
        "    progress_log = {}\n",
        "\n",
        "# Fetch works for each author and organize them under authors\n",
        "authors_with_works = []\n",
        "for member in members:\n",
        "    name = member[\"name\"]\n",
        "    openalex_ids = member.get(\"openalex_id\")\n",
        "    all_works = []\n",
        "\n",
        "    if name in progress_log and progress_log[name]:\n",
        "        print(f\"Skipping {name}, already completed.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Fetching works for {name}...\")\n",
        "\n",
        "    try:\n",
        "        if isinstance(openalex_ids, list):\n",
        "            for openalex_id in openalex_ids:\n",
        "                author_id = openalex_id.split(\"/\")[-1]  # Extract author ID\n",
        "                all_works.extend(fetch_author_works(author_id))\n",
        "        elif isinstance(openalex_ids, str):\n",
        "            author_id = openalex_ids.split(\"/\")[-1]\n",
        "            all_works.extend(fetch_author_works(author_id))\n",
        "\n",
        "        authors_with_works.append({\n",
        "            \"name\": name,\n",
        "            \"openalex_id\": openalex_ids,\n",
        "            \"works\": all_works\n",
        "        })\n",
        "\n",
        "        # Mark the author as successfully processed\n",
        "        progress_log[name] = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {name}: {e}\")\n",
        "        progress_log[name] = False  # Mark as failed\n",
        "\n",
        "    # Save progress after each author\n",
        "    with open(progress_log_file, \"w\") as log_file:\n",
        "        json.dump(progress_log, log_file, indent=4)\n",
        "\n",
        "# Save the filtered data to a JSON file\n",
        "with open(output_json_file, \"w\") as outfile:\n",
        "    json.dump(authors_with_works, outfile, indent=4)\n",
        "print(f\"Filtered works data saved to JSON: {output_json_file}\")\n",
        "\n",
        "# Convert to CSV for easier inspection\n",
        "csv_data = []\n",
        "for author in authors_with_works:\n",
        "    for work in author[\"works\"]:\n",
        "        csv_data.append({\n",
        "            \"author_name\": author[\"name\"],\n",
        "            \"author_openalex_id\": author[\"openalex_id\"],\n",
        "            \"work_id\": work[\"id\"],\n",
        "            \"work_title\": work[\"title\"],\n",
        "            \"doi\": work[\"doi\"],\n",
        "            \"publication_year\": work[\"publication_year\"],\n",
        "            \"cited_by_count\": work[\"cited_by_count\"],\n",
        "            \"field\": work[\"primary_topic\"].get(\"field\", {}).get(\"display_name\", \"\"),\n",
        "            \"subfield\": work[\"primary_topic\"].get(\"subfield\", {}).get(\"display_name\", \"\"),\n",
        "            \"domain\": work[\"primary_topic\"].get(\"domain\", {}).get(\"display_name\", \"\"),\n",
        "            \"concepts\": \", \".join([concept[\"display_name\"] for concept in work[\"concepts\"]]),\n",
        "            \"open_access_status\": work.get(\"open_access\", {}).get(\"oa_status\", \"\"),\n",
        "            \"sustainable_development_goals\": \", \".join([sdg[\"display_name\"] for sdg in work.get(\"sustainable_development_goals\", [])]),\n",
        "            \"referenced_works_count\": len(work.get(\"referenced_works\", []))\n",
        "        })\n",
        "\n",
        "pd.DataFrame(csv_data).to_csv(output_csv_file, index=False)\n",
        "print(f\"Filtered works data saved to CSV: {output_csv_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBDu2eWzrOcB",
        "outputId": "51140353-93db-4aba-cabf-9ce33cad77cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching works for Can Fuat Delale...\n",
            "Fetching works for Mehmet Özdoğan...\n",
            "Fetching works for Aslıhan Yener...\n",
            "Fetching works for M. Ali Alpar...\n",
            "Fetching works for Ersin Göğüş...\n",
            "Fetching works for Feryal Özel...\n",
            "Fetching works for Ethem Alpaydın...\n",
            "Fetching works for Lale Akarun...\n",
            "Fetching works for Tamer Özsu...\n",
            "Fetching works for İzak Benbasat...\n",
            "Fetching works for Attila Gürsoy...\n",
            "Fetching works for Pekcan Ungan...\n",
            "Fetching works for Canan Atılgan...\n",
            "Fetching works for İvet Bahar...\n",
            "Fetching works for Mustafa Tekin...\n",
            "Fetching works for Şermin Genç...\n",
            "Fetching works for Mehmet Öztürk...\n",
            "Fetching works for Vasıf Hasırcı...\n",
            "Fetching works for Berrin Tansel...\n",
            "Fetching works for Nilsun İnce...\n",
            "Fetching works for Orhan Yenigün...\n",
            "Fetching works for Delia Sponza...\n",
            "Fetching works for Mehmet Kobya...\n",
            "Fetching works for İdil Arslan Alaton...\n",
            "Fetching works for Derin Orhon...\n",
            "Fetching works for İhsan Çalış...\n",
            "Fetching works for Hüsnü Can Başer...\n",
            "Fetching works for Sedat Ölçer...\n",
            "Fetching works for Tolga Mete Duman...\n",
            "Fetching works for Enis Çetin...\n",
            "Fetching works for Ergin Atalar...\n",
            "Fetching works for Bülent Sankur...\n",
            "Fetching works for Erol Gelenbe...\n",
            "Fetching works for Erdal Panayırcı...\n",
            "Fetching works for Özgür Barış Akan...\n",
            "Fetching works for Alper Demir...\n",
            "Fetching works for Murat Tekalp...\n",
            "Fetching works for Hakan Ürey...\n",
            "Fetching works for Asuman Özdağlar...\n",
            "Fetching works for Elza Erkip...\n",
            "Fetching works for Haluk Külah...\n",
            "Fetching works for Aydın Alatan...\n",
            "Fetching works for Murat Uysal...\n",
            "Fetching works for Aylin Yener...\n",
            "Fetching works for Selim Ünlü...\n",
            "Fetching works for Fikri Karaesmen...\n",
            "Fetching works for Murat Köksalan...\n",
            "Fetching works for Pınar Keskinocak...\n",
            "Fetching works for Sibel Salman...\n",
            "Fetching works for Gürol Irzık...\n",
            "Fetching works for Alpan Bek...\n",
            "Fetching works for Yalçın Elerman...\n",
            "Fetching works for Ercan Alp...\n",
            "Fetching works for Emrah Kalemci...\n",
            "Fetching works for Ömer İlday...\n",
            "Fetching works for Metin Gürses...\n",
            "Fetching works for Bilal Tanatar...\n",
            "Fetching works for Oğuz Gülseren...\n",
            "Fetching works for Metin Arık...\n",
            "Fetching works for Özhan Özatay...\n",
            "Fetching works for Mehmet Erbudak...\n",
            "Fetching works for Uğur Tırnaklı...\n",
            "Fetching works for Ayşe Erzan...\n",
            "Fetching works for İsmail Hakkı Duru...\n",
            "Fetching works for Ramazan Tuğrul Senger...\n",
            "Fetching works for Nejat Bulut...\n",
            "Fetching works for Önder Pekcan...\n",
            "Fetching works for Ali Mostafazadeh...\n",
            "Fetching works for Alphan Sennaroğlu...\n",
            "Fetching works for Özgür Müstecaplıoğlu...\n",
            "Fetching works for Tekin Dereli...\n",
            "Fetching works for Mahmut Hortaçsu...\n",
            "Fetching works for Serdar Öğüt...\n",
            "Fetching works for Altan Baykal...\n",
            "Fetching works for Ahmet Oral...\n",
            "Fetching works for Altuğ Özpineci...\n",
            "Fetching works for Mehmet Tomak...\n",
            "Fetching works for İnanç Adagideli...\n",
            "Fetching works for Cihan Saçlıoğlu...\n",
            "Fetching works for Şinasi Ellialtıoğlu...\n",
            "Fetching works for Ergin Sezgin...\n",
            "Fetching works for Mete Atatüre...\n",
            "Fetching works for Baha Balantekin...\n",
            "Fetching works for Refik Kortan...\n",
            "Fetching works for Umut Gürsoy...\n",
            "Fetching works for Umran İnan...\n",
            "Fetching works for Tahir Çağın...\n",
            "Fetching works for Türkan Haliloğlu...\n",
            "Fetching works for Özlem Keskin...\n",
            "Fetching works for Ünal Tekinalp...\n",
            "Fetching works for Bertil Emrah Oder...\n",
            "Fetching works for Yeşim Atamer...\n",
            "Fetching works for Refet Gürkaynak...\n",
            "Fetching works for Ayşe Buğra Kavala...\n",
            "Fetching works for Tayfun Sönmez...\n",
            "Fetching works for M. Utku Ünver...\n",
            "Fetching works for Dani Rodrik...\n",
            "Fetching works for Erinç Yeldan...\n",
            "Fetching works for Kamil Yılmaz...\n",
            "Fetching works for Daron Acemoğlu...\n",
            "Fetching works for Erol Taymaz...\n",
            "Fetching works for Ahmet Alkan...\n",
            "Fetching works for Nezih Güner...\n",
            "Fetching works for Ufuk Akçiğit...\n",
            "Fetching works for Ayşe İmrohoroğlu...\n",
            "Fetching works for Şevket Pamuk...\n",
            "Fetching works for Hakkı Polat Gülkan...\n",
            "Fetching works for Bülent Mengüç...\n",
            "Fetching works for Cahit Helvacı...\n",
            "Fetching works for Attila Çiner...\n",
            "Fetching works for Namık Çağatay...\n",
            "Fetching works for Erdin Bozkurt...\n",
            "Fetching works for Ali Polat...\n",
            "Fetching works for Okan Tüysüz...\n",
            "Fetching works for Ziyadin Çakır...\n",
            "Fetching works for Seda Keskin...\n",
            "Fetching works for Şefik Süzer...\n",
            "Fetching works for Ömer Dağ...\n",
            "Fetching works for Viktorya Aviyente...\n",
            "Fetching works for Engin Umut Akkaya...\n",
            "Fetching works for Olgun Güven...\n",
            "Fetching works for Adil Denizli...\n",
            "Fetching works for Ersin Yurtsever...\n",
            "Fetching works for Can Erkey...\n",
            "Fetching works for Nesrin Hasırcı...\n",
            "Fetching works for Saim Özkar...\n",
            "Fetching works for Cihangir Tanyeli...\n",
            "Fetching works for Levent Toppare...\n",
            "Fetching works for İlhan Aksay...\n",
            "Fetching works for Çağatay Başdoğan...\n",
            "Fetching works for Nevzat Özgüven...\n",
            "Fetching works for M. Pınar Mengüç...\n",
            "Fetching works for Ali Erdemir...\n",
            "Fetching works for Ergün Yalçın...\n",
            "Fetching works for Cem Yalçın Yıldırım...\n",
            "Fetching works for Ali Nesin...\n",
            "Fetching works for Ali Ülger...\n",
            "Fetching works for Hüsnü Ata Erbay...\n",
            "Fetching works for Halil Mete Soner...\n",
            "Fetching works for Hilmi Demiray...\n",
            "Fetching works for İlhan Tekeli...\n",
            "Fetching works for Zeynep Çelik...\n",
            "Fetching works for İhsan Gürsel...\n",
            "Fetching works for Tamer Önder...\n",
            "Fetching works for Aydoğan Özcan...\n",
            "Fetching works for Metin Sitti...\n",
            "Fetching works for Kamil Uğurbil...\n",
            "Fetching works for Levent Demirel...\n",
            "Fetching works for Ayhan Aksu Koç...\n",
            "Fetching works for Ayşe Nuray Karancı...\n",
            "Fetching works for Gün Şemin...\n",
            "Fetching works for Zeynep Aycan...\n",
            "Fetching works for Mehmet Eskin...\n",
            "Fetching works for Sami Gülgöz...\n",
            "Fetching works for Nebi Sümer...\n",
            "Fetching works for Ayhan Kaya...\n",
            "Fetching works for Yeşim Arat...\n",
            "Fetching works for Ali Çarkoğlu...\n",
            "Fetching works for Ahmet İçduygu...\n",
            "Fetching works for Ersin Kalaycıoğlu...\n",
            "Fetching works for Yılmaz Esmer...\n",
            "Fetching works for Çağlar Keyder...\n",
            "Fetching works for Ayşe Öncü...\n",
            "Fetching works for Yasemin Soysal...\n",
            "Fetching works for Deniz Kandiyoti...\n",
            "Fetching works for Edhem Eldem...\n",
            "Fetching works for İsenbike Togan...\n",
            "Fetching works for Reşat Kasaba...\n",
            "Fetching works for Moshe Arditi...\n",
            "Fetching works for Okan Bülent Yıldız...\n",
            "Fetching works for Murat Akova...\n",
            "Fetching works for Önder Ergönül...\n",
            "Fetching works for İsmail Hakkı Ulus...\n",
            "Fetching works for Mustafa İlhan...\n",
            "Fetching works for Hakan Orer...\n",
            "Fetching works for Kemal Sıtkı Türker...\n",
            "Fetching works for Berrak Çağlayan Yeğen...\n",
            "Fetching works for Ramazan İdilman...\n",
            "Fetching works for Feza Remzi...\n",
            "Fetching works for Murat Karaçorlu...\n",
            "Fetching works for Meral Beksaç...\n",
            "Fetching works for Reyhan Küçükkaya...\n",
            "Fetching works for Lale Tokgözoğlu...\n",
            "Fetching works for Mustafa Arıcı...\n",
            "Fetching works for Deniz Kırık...\n",
            "Fetching works for Cumhur Ertekin...\n",
            "Fetching works for Turgay Dalkara...\n",
            "Fetching works for Aksel Siva...\n",
            "Fetching works for Marsel Meşulam...\n",
            "Fetching works for Necmettin Pamir...\n",
            "Fetching works for Şevket Ruacan...\n",
            "Fetching works for Tarık Tihan...\n",
            "Fetching works for Seza Özen...\n",
            "Fetching works for Okan Akhan...\n",
            "Fetching works for Ahmet Gül...\n",
            "Fetching works for Hasan Yazıcı...\n",
            "Fetching works for Hülya Kayserili...\n",
            "Fetching works for Alp Can...\n",
            "Fetching works for Sevtap Arıkan Akdağlı...\n",
            "Fetching works for Haluk Özen...\n",
            "Fetching works for Mehmet Toner...\n",
            "Fetching works for Naci Görür...\n",
            "Fetching works for Yücel Yılmaz...\n",
            "Fetching works for İsmail Çakmak...\n",
            "Fetching works for Cem Ersoy...\n",
            "Fetching works for Seda Ertaç...\n",
            "Fetching works for Ece Göztepe...\n",
            "Fetching works for Ali Hortaçsu...\n",
            "Fetching works for Yasemin Gürsoy Özdemir...\n",
            "Fetching works for Kübra Doğan Yenisey...\n",
            "Fetching works for Ali İzzet Tekcan...\n",
            "Fetching works for Canan Sümer...\n",
            "Fetching works for Serpil Sayın...\n",
            "Fetching works for Metin Muradoğlu...\n",
            "Fetching works for Murat Mungan...\n",
            "Fetching works for Aylin Küntay...\n",
            "Fetching works for Semih Koray...\n",
            "Fetching works for Alev Devrim Güçlü...\n",
            "Fetching works for Nesim Erkip...\n",
            "Fetching works for Başak Çalı...\n",
            "Fetching works for Burcu Balçık...\n",
            "Fetching works for Orhan Arıkan...\n",
            "Fetching works for Robert P. Langlands...\n",
            "Fetching works for Edward Witten...\n",
            "Fetching works for Hakkı Erdal Akalın...\n",
            "Fetching works for Takhmasib Aliev...\n",
            "Fetching works for Orhan Altan...\n",
            "Fetching works for Yoshinori Asakawa...\n",
            "Fetching works for Itzhak Bars...\n",
            "Fetching works for Dame Jocelyn Bell Burnell...\n",
            "Fetching works for Şeyla Benhabib...\n",
            "Fetching works for Edouard Brezin...\n",
            "Fetching works for Martin Chalfie...\n",
            "Fetching works for Ayşe Çağlar...\n",
            "Fetching works for Peter A. Diamond...\n",
            "Fetching works for Üstün Ergüder...\n",
            "Fetching works for Suraiya Faroqhi...\n",
            "Fetching works for Dagfinn Follesdal...\n",
            "Fetching works for Teo Grünberg...\n",
            "Fetching works for Dimitri Gutas...\n",
            "Fetching works for Ayla Zırh Gürsoy...\n",
            "Fetching works for Avedis Hacınlıyan...\n",
            "Fetching works for Philip Kitcher...\n",
            "Fetching works for İoanna Kuçuradi...\n",
            "Fetching works for Timur Kuran...\n",
            "Fetching works for Joel Lebowitz...\n",
            "Fetching works for Dan P. McKenzie...\n",
            "Fetching works for Sanjit K. Mitra...\n",
            "Fetching works for Cumhur Öner...\n",
            "Fetching works for John Polanyi...\n",
            "Fetching works for Atta-ur Rahman...\n",
            "Fetching works for Lord Martin Rees...\n",
            "Fetching works for Sir Adam Roberts...\n",
            "Fetching works for Hikmet Sarı...\n",
            "Fetching works for İskender Sayek...\n",
            "Fetching works for Zehra Sayers...\n",
            "Fetching works for İzzettin Silier...\n",
            "Fetching works for Christopher Llewellyn Smith...\n",
            "Fetching works for Günter Stock...\n",
            "Fetching works for Alessandro Vespignani...\n",
            "Fetching works for Dame Hellen Wallace...\n",
            "Fetching works for David Pines...\n",
            "Fetching works for Philip W. Anderson...\n",
            "Fetching works for Erdal Yıldırım...\n",
            "Fetching works for Miral Dizdaroğlu...\n",
            "Fetching works for Fuat Keyman...\n",
            "Fetching works for Durmuş Ali Demir...\n",
            "Fetching works for Zafer Toprak...\n",
            "Fetching works for Ayhan Ulubelen...\n",
            "Fetching works for Hamit Fişek...\n",
            "Fetching works for Rahmi Güven...\n",
            "Fetching works for Güven Arsebük...\n",
            "Fetching works for Çiğdem Kağıtçıbaşı...\n",
            "Fetching works for Yücel Kanpolat...\n",
            "Fetching works for Tosun Terzioğlu...\n",
            "Fetching works for Namık Kemal Pak...\n",
            "Filtered works data saved to JSON: ba_works_filtered.json\n",
            "Filtered works data saved to CSV: ba_works_filtered.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/ba_works_filtered.json\"  # Replace with your distorted JSON file path\n",
        "output_file = \"ba_works_filtered_restored.json\"  # Restored output file\n",
        "\n",
        "# Load the distorted JSON file\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "# Decode Unicode escape sequences\n",
        "def decode_unicode(data):\n",
        "    if isinstance(data, dict):\n",
        "        return {key: decode_unicode(value) for key, value in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [decode_unicode(item) for item in data]\n",
        "    elif isinstance(data, str):\n",
        "        try:\n",
        "            # Attempt to decode the string\n",
        "            return data.encode('latin1').decode('utf-8')\n",
        "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "            # Return the original string if decoding fails\n",
        "            return data\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "restored_data = decode_unicode(data)\n",
        "\n",
        "# Save the restored JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    json.dump(restored_data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Restored file saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFqK4xagFQNH",
        "outputId": "ce28d376-01f9-4d6a-b382-4a7d5d8549e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored file saved to ba_works_filtered_restored.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/ba_works_filtered.csv\"  # Replace with your distorted CSV file path\n",
        "output_file = \"ba_works_filtered_restored.csv\"  # Restored output file\n",
        "\n",
        "# Load the distorted CSV file\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Decode Unicode escape sequences in strings\n",
        "def decode_unicode_string(value):\n",
        "    if isinstance(value, str):  # Ensure the value is a string before decoding\n",
        "        try:\n",
        "            return value.encode('latin1').decode('utf-8')\n",
        "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "            return value  # Return the original value if decoding fails\n",
        "    return value  # Return the value as-is if it's not a string\n",
        "\n",
        "# Apply the Unicode decoding function to all string cells in the DataFrame\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':  # Apply only to object (string) columns\n",
        "        df[col] = df[col].apply(decode_unicode_string)\n",
        "\n",
        "# Save the restored CSV file\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Restored file saved to {output_file}\")"
      ],
      "metadata": {
        "id": "v7jMkrh4FYyg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2338c94d-7537-4962-f550-523d82be26c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored file saved to ba_works_filtered_restored.csv\n"
          ]
        }
      ]
    }
  ]
}
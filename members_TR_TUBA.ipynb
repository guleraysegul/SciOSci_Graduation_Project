{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tmAm6HOBnzd",
        "outputId": "db4991f6-bc49-4fd3-eb02-324ac2e0bf40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXgfPnfhBPJu",
        "outputId": "531039bf-80b8-4f9a-85e7-03d5b67be603"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching OpenAlex Author ID for Muzaffer Şeker...\n",
            "Fetching OpenAlex Author ID for Atilla Abdulkadiroğlu...\n",
            "Fetching OpenAlex Author ID for Mustafa Acar...\n",
            "Fetching OpenAlex Author ID for Ahmet Cevat Acar...\n",
            "Fetching OpenAlex Author ID for Alparslan Açıkgenç...\n",
            "Fetching OpenAlex Author ID for Nazmi Volkan Adsay...\n",
            "Fetching OpenAlex Author ID for Ali Ekber Akgün...\n",
            "Fetching OpenAlex Author ID for M. İrşadi Aksun...\n",
            "Fetching OpenAlex Author ID for Yasin Aktay...\n",
            "Fetching OpenAlex Author ID for Şener Aktürk...\n",
            "Fetching OpenAlex Author ID for Ali Akyıldız...\n",
            "Fetching OpenAlex Author ID for Mehmet Hakkı Alma...\n",
            "Fetching OpenAlex Author ID for Meliha Altunışık...\n",
            "Fetching OpenAlex Author ID for M. Fatih Andı...\n",
            "Fetching OpenAlex Author ID for Mustafa Reşat Apak...\n",
            "Fetching OpenAlex Author ID for Erol Arcaklıoğlu...\n",
            "Fetching OpenAlex Author ID for Metin Arık...\n",
            "Fetching OpenAlex Author ID for Erdal Arıkan...\n",
            "Fetching OpenAlex Author ID for Hüseyin Arslan...\n",
            "Fetching OpenAlex Author ID for Ercümend Arvas...\n",
            "Fetching OpenAlex Author ID for Messoud Ashina...\n",
            "Fetching OpenAlex Author ID for Abdullah Atalar...\n",
            "Fetching OpenAlex Author ID for Ali Tayfun Atay...\n",
            "Fetching OpenAlex Author ID for Abdurrahman Atçıl...\n",
            "Fetching OpenAlex Author ID for Cenk Ayata...\n",
            "Fetching OpenAlex Author ID for Orhan Aydın...\n",
            "Fetching OpenAlex Author ID for Mehmet Emin Aydın...\n",
            "Fetching OpenAlex Author ID for Ahmet Faruk Aysan...\n",
            "Fetching OpenAlex Author ID for Mehmet Baç...\n",
            "Fetching OpenAlex Author ID for Ali Müfit Bahadır...\n",
            "Fetching OpenAlex Author ID for Sezgin Bakırdere...\n",
            "Fetching OpenAlex Author ID for Ali Balcı...\n",
            "Fetching OpenAlex Author ID for Metin Balcı...\n",
            "Fetching OpenAlex Author ID for Erden Banoğlu...\n",
            "Fetching OpenAlex Author ID for Ertuğrul Başar...\n",
            "Fetching OpenAlex Author ID for Yıldız Bayazıtoğlu...\n",
            "Fetching OpenAlex Author ID for Adrian Bejan...\n",
            "Fetching OpenAlex Author ID for Özer  Bekaroğlu...\n",
            "Fetching OpenAlex Author ID for Meral Beksaç...\n",
            "Fetching OpenAlex Author ID for Hayrunnisa Bolay Belen...\n",
            "Fetching OpenAlex Author ID for A. Nihat Berker...\n",
            "Fetching OpenAlex Author ID for Zulfiqar Ahmed Bhutta...\n",
            "Fetching OpenAlex Author ID for Pınar Bilgin...\n",
            "Fetching OpenAlex Author ID for Erhan Bişkin...\n",
            "Fetching OpenAlex Author ID for Mehmet Bulut...\n",
            "Fetching OpenAlex Author ID for Ömer Çaha...\n",
            "Fetching OpenAlex Author ID for Ömer Ziya Cebeci...\n",
            "Fetching OpenAlex Author ID for Mehmet Çelik...\n",
            "Fetching OpenAlex Author ID for Gülfettin Çelik...\n",
            "Fetching OpenAlex Author ID for Bekir Çetinkaya...\n",
            "Fetching OpenAlex Author ID for Abdulkadir Çevik...\n",
            "Fetching OpenAlex Author ID for M.talha Çiçek...\n",
            "Fetching OpenAlex Author ID for Mustafa Çiçekler...\n",
            "Fetching OpenAlex Author ID for Ümit Cizre...\n",
            "Fetching OpenAlex Author ID for Salim Çıracı...\n",
            "Fetching OpenAlex Author ID for Amnon Cohen...\n",
            "Fetching OpenAlex Author ID for Turgay Dalkara...\n",
            "Fetching OpenAlex Author ID for Prof. Dr. Robert Dankoff...\n",
            "Fetching OpenAlex Author ID for Hilmi Volkan Demir...\n",
            "Fetching OpenAlex Author ID for Şeref Demirayak...\n",
            "Fetching OpenAlex Author ID for Taner Demirer...\n",
            "Fetching OpenAlex Author ID for Bilge Demirköz...\n",
            "Fetching OpenAlex Author ID for Adil Denizli...\n",
            "Fetching OpenAlex Author ID for Tekin Dereli...\n",
            "Fetching OpenAlex Author ID for Stefan Dimitrov...\n",
            "Fetching OpenAlex Author ID for İbrahim Dinçer...\n",
            "Fetching OpenAlex Author ID for Seydi Doğan...\n",
            "Fetching OpenAlex Author ID for Timur Doğu...\n",
            "Fetching OpenAlex Author ID for Oktay Duman...\n",
            "Fetching OpenAlex Author ID for Engin Durgun...\n",
            "Fetching OpenAlex Author ID for Nazım Ekren...\n",
            "Fetching OpenAlex Author ID for Muzaffer Elmas...\n",
            "Fetching OpenAlex Author ID for Feridun M. Emecen...\n",
            "Fetching OpenAlex Author ID for Sevim Ercan...\n",
            "Fetching OpenAlex Author ID for Bayram Zafer Erdoğan...\n",
            "Fetching OpenAlex Author ID for Mustafa Erdoğan...\n",
            "Fetching OpenAlex Author ID for Özcan Erel...\n",
            "Fetching OpenAlex Author ID for Mustafa Evren Erşahin...\n",
            "Fetching OpenAlex Author ID for Mustafa Ersöz...\n",
            "Fetching OpenAlex Author ID for Ayşe Selçuk Esenbel...\n",
            "Fetching OpenAlex Author ID for Zafer Evis...\n",
            "Fetching OpenAlex Author ID for Carter Vaughn Findley...\n",
            "Fetching OpenAlex Author ID for Tülin Gençöz...\n",
            "Fetching OpenAlex Author ID for Vural Gökmen...\n",
            "Fetching OpenAlex Author ID for Nilüfer Göle...\n",
            "Fetching OpenAlex Author ID for Ahmet Gül...\n",
            "Fetching OpenAlex Author ID for Aydın Gülan...\n",
            "Fetching OpenAlex Author ID for İlhami Gülçin...\n",
            "Fetching OpenAlex Author ID for Fatih Gültekin...\n",
            "Fetching OpenAlex Author ID for Naci Gündoğan...\n",
            "Fetching OpenAlex Author ID for Şinasi Gündüz...\n",
            "Fetching OpenAlex Author ID for Mesut Güner...\n",
            "Fetching OpenAlex Author ID for Bahar Güntekin...\n",
            "Fetching OpenAlex Author ID for K. Arzum  Erdem Gürsan...\n",
            "Fetching OpenAlex Author ID for Esra Çapanoğlu Güven...\n",
            "Fetching OpenAlex Author ID for Wael B. Hallaq...\n",
            "Fetching OpenAlex Author ID for Gabor Hamza...\n",
            "Fetching OpenAlex Author ID for M. Şükrü Hanioğlu...\n",
            "Fetching OpenAlex Author ID for İbrahim Hatiboğlu...\n",
            "Fetching OpenAlex Author ID for Ali Adnan Hayaloğlu...\n",
            "Fetching OpenAlex Author ID for Kadir Mutlu Hayran...\n",
            "Fetching OpenAlex Author ID for Arif Hepbaşlı...\n",
            "Fetching OpenAlex Author ID for Metin Heper...\n",
            "Fetching OpenAlex Author ID for Gökhan Hotamışlıgil...\n",
            "Fetching OpenAlex Author ID for Orhan İçelli...\n",
            "Fetching OpenAlex Author ID for Ekmeleddin İhsanoğlu...\n",
            "Fetching OpenAlex Author ID for Umran Savaş İnan...\n",
            "Fetching OpenAlex Author ID for Erik Jeppesen...\n",
            "Fetching OpenAlex Author ID for Shuanggen Jin...\n",
            "Fetching OpenAlex Author ID for Mustafa S. Kaçalin...\n",
            "Fetching OpenAlex Author ID for Cemal Kafadar...\n",
            "Fetching OpenAlex Author ID for Esin Kahya...\n",
            "Fetching OpenAlex Author ID for Sadık Kakaç...\n",
            "Fetching OpenAlex Author ID for Münci Kalayoğlu...\n",
            "Fetching OpenAlex Author ID for Mahmut Kandemir...\n",
            "Fetching OpenAlex Author ID for Emin Kansu...\n",
            "Fetching OpenAlex Author ID for Muhsin Kar...\n",
            "Fetching OpenAlex Author ID for İsmail Kara...\n",
            "Fetching OpenAlex Author ID for Ramazan Karakuzu...\n",
            "Fetching OpenAlex Author ID for Tanju Karanfil...\n",
            "Fetching OpenAlex Author ID for Taşkın Kavzoğlu...\n",
            "Fetching OpenAlex Author ID for Kamil Kaygusuz...\n",
            "Fetching OpenAlex Author ID for Okyay Kaynak...\n",
            "Fetching OpenAlex Author ID for Ali Keleş...\n",
            "Fetching OpenAlex Author ID for Hasan Fahrettin Keleştemur...\n",
            "Fetching OpenAlex Author ID for Halit Keskin...\n",
            "Fetching OpenAlex Author ID for Mary-Claire King...\n",
            "Fetching OpenAlex Author ID for Alper Kiraz...\n",
            "Fetching OpenAlex Author ID for Özgür Kişi...\n",
            "Fetching OpenAlex Author ID for Ahmet Saim Kılavuz...\n",
            "Fetching OpenAlex Author ID for Ertuğrul Kılıç...\n",
            "Fetching OpenAlex Author ID for Zeynel Kılıç...\n",
            "Fetching OpenAlex Author ID for Atıf Koca...\n",
            "Fetching OpenAlex Author ID for Mustafa Verşan Kök...\n",
            "Fetching OpenAlex Author ID for Ayşegül Komsuoğlu...\n",
            "Fetching OpenAlex Author ID for Feza Korkusuz...\n",
            "Fetching OpenAlex Author ID for Ali Koşar...\n",
            "Fetching OpenAlex Author ID for İsmail Koyuncu...\n",
            "Fetching OpenAlex Author ID for Ömer Küçük...\n",
            "Fetching OpenAlex Author ID for Filiz Kuralay...\n",
            "Fetching OpenAlex Author ID for Hamza Kurt...\n",
            "Fetching OpenAlex Author ID for İlhan Kutluer...\n",
            "Fetching OpenAlex Author ID for Bold Luvsandorj...\n",
            "Fetching OpenAlex Author ID for Hasan Mandal...\n",
            "Fetching OpenAlex Author ID for Önder Metin...\n",
            "Fetching OpenAlex Author ID for Ali Mostafazadeh...\n",
            "Fetching OpenAlex Author ID for Şaban Nazlıoğlu...\n",
            "Fetching OpenAlex Author ID for Ahmet Yaşar Ocak...\n",
            "Fetching OpenAlex Author ID for Oğuz Okay...\n",
            "Fetching OpenAlex Author ID for Ziya Öniş...\n",
            "Fetching OpenAlex Author ID for İlkay Erdoğan Orhan...\n",
            "Fetching OpenAlex Author ID for Necati Örmeci...\n",
            "Fetching OpenAlex Author ID for İlber Ortaylı...\n",
            "Fetching OpenAlex Author ID for Mahmut Özacar...\n",
            "Fetching OpenAlex Author ID for Haldun M. Özaktaş...\n",
            "Fetching OpenAlex Author ID for Ekmel Özbay...\n",
            "Fetching OpenAlex Author ID for Yusuf Ziya Özcan...\n",
            "Fetching OpenAlex Author ID for Hasan Tayfun Özçelik...\n",
            "Fetching OpenAlex Author ID for Ali Özer...\n",
            "Fetching OpenAlex Author ID for İzzet Özgenç...\n",
            "Fetching OpenAlex Author ID for Cengiz Sinan Özkan...\n",
            "Fetching OpenAlex Author ID for Mehmed Özkan...\n",
            "Fetching OpenAlex Author ID for Saim Özkar...\n",
            "Fetching OpenAlex Author ID for Ali Rıza Özkaya...\n",
            "Fetching OpenAlex Author ID for Mehmet Öztürk...\n",
            "Fetching OpenAlex Author ID for Özcan Öztürk...\n",
            "Fetching OpenAlex Author ID for İzzet Öztürk...\n",
            "Fetching OpenAlex Author ID for Mehmet Akif Öztürk...\n",
            "Fetching OpenAlex Author ID for Mustafa Serdar Palabıyık...\n",
            "Fetching OpenAlex Author ID for Viorel Panaite...\n",
            "Fetching OpenAlex Author ID for Hakan Parlakpınar...\n",
            "Fetching OpenAlex Author ID for Erol Pehlivan...\n",
            "Fetching OpenAlex Author ID for Necati Polat...\n",
            "Fetching OpenAlex Author ID for Jamil Ragep...\n",
            "Fetching OpenAlex Author ID for Jeffry David Sachs...\n",
            "Fetching OpenAlex Author ID for Mustafa Safran...\n",
            "Fetching OpenAlex Author ID for Bahri Şahin...\n",
            "Fetching OpenAlex Author ID for Kazım Şahin...\n",
            "Fetching OpenAlex Author ID for Fikrettin Şahin...\n",
            "Fetching OpenAlex Author ID for Mustafa Şahmaran...\n",
            "Fetching OpenAlex Author ID for Bekir Salih...\n",
            "Fetching OpenAlex Author ID for Aziz Sancar...\n",
            "Fetching OpenAlex Author ID for M. A. Yekta Saraç...\n",
            "Fetching OpenAlex Author ID for Niyazi Serdar Sarıçiftçi...\n",
            "Fetching OpenAlex Author ID for Hüseyin Sarıoğlu...\n",
            "Fetching OpenAlex Author ID for Ekrem Savaş...\n",
            "Fetching OpenAlex Author ID for Mustafa Solak...\n",
            "Fetching OpenAlex Author ID for Mustafa Soylak...\n",
            "Fetching OpenAlex Author ID for Yunus Söylet...\n",
            "Fetching OpenAlex Author ID for Joseph Jao-Yiu Sung...\n",
            "Fetching OpenAlex Author ID for Ahmet Tabakoğlu...\n",
            "Fetching OpenAlex Author ID for Candan Tamerler...\n",
            "Fetching OpenAlex Author ID for Nusret Tan...\n",
            "Fetching OpenAlex Author ID for Bilal Tanatar...\n",
            "Fetching OpenAlex Author ID for Feridun Cahit Tanyel...\n",
            "Fetching OpenAlex Author ID for Ekrem Tatoğlu...\n",
            "Fetching OpenAlex Author ID for Ahmet Murat Tekalp...\n",
            "Fetching OpenAlex Author ID for İsenbike Togan...\n",
            "Fetching OpenAlex Author ID for Haluk Aydın Topaloğlu...\n",
            "Fetching OpenAlex Author ID for S. Ali Tuncel...\n",
            "Fetching OpenAlex Author ID for Mustafa Tüzen...\n",
            "Fetching OpenAlex Author ID for Saffet Tüzgen...\n",
            "Fetching OpenAlex Author ID for Mehmet Fatih Uçkun...\n",
            "Fetching OpenAlex Author ID for İsmail Hakkı Ulus...\n",
            "Fetching OpenAlex Author ID for Joseph Wang...\n",
            "Fetching OpenAlex Author ID for Omar Yaghi...\n",
            "Fetching OpenAlex Author ID for M. Cemal Yalabık...\n",
            "Fetching OpenAlex Author ID for Halit Yanıkkaya...\n",
            "Fetching OpenAlex Author ID for Gazi Yaşargil...\n",
            "Fetching OpenAlex Author ID for Serap Yazıcı...\n",
            "Fetching OpenAlex Author ID for Okan Zafer Yeşilel...\n",
            "Fetching OpenAlex Author ID for Jackie Y. Ying...\n",
            "Fetching OpenAlex Author ID for Hüseyin Yıldırım...\n",
            "Fetching OpenAlex Author ID for Ali Rıza Yıldız...\n",
            "Fetching OpenAlex Author ID for Mehmet Emin Yılmaz...\n",
            "Fetching OpenAlex Author ID for Mustafa Yücel...\n",
            "Fetching OpenAlex Author ID for Ahmet Nuri Yurdusev...\n",
            "Fetching OpenAlex Author ID for Mehmet Zahmakıran...\n",
            "Fetching OpenAlex Author ID for Erkan Zergeroğlu...\n",
            "Fetching OpenAlex Author ID for Bülent Zülfikar...\n",
            "Author IDs saved to 'tuba_author_ids.json'.\n",
            "Author IDs also saved to 'tuba_author_ids.csv'.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file_path = \"/content/tuba_members_all.json\"  # Replace with actual file path\n",
        "output_file_path = \"tuba_author_ids.json\"\n",
        "\n",
        "# Load KNAW members JSON\n",
        "with open(input_file_path, 'r', encoding='utf-8') as file:\n",
        "    members = json.load(file)\n",
        "\n",
        "# OpenAlex API endpoint\n",
        "openalex_base_url = \"https://api.openalex.org/authors\"\n",
        "\n",
        "# Function to query OpenAlex\n",
        "def get_openalex_author_id(name):\n",
        "    try:\n",
        "        response = requests.get(openalex_base_url, params={\"search\": name})\n",
        "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
        "        results = response.json()\n",
        "\n",
        "        if \"results\" in results and results[\"results\"]:\n",
        "            # Take the first match (or apply more filters if needed)\n",
        "            first_result = results[\"results\"][0]\n",
        "            return {\n",
        "                \"author_id\": first_result.get(\"id\"),\n",
        "                \"name\": first_result.get(\"display_name\"),\n",
        "                \"works_count\": first_result.get(\"works_count\", 0),\n",
        "                \"cited_by_count\": first_result.get(\"cited_by_count\", 0),\n",
        "            }\n",
        "        else:\n",
        "            return None  # No match found\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching author ID for {name}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process each member to fetch OpenAlex Author ID\n",
        "results = []\n",
        "for member in members:\n",
        "    print(f\"Fetching OpenAlex Author ID for {member['Name']}...\")\n",
        "    author_data = get_openalex_author_id(member[\"Name\"])\n",
        "    if author_data:\n",
        "        results.append({\n",
        "            \"Name\": member[\"Name\"],\n",
        "            \"Institution\": member[\"Institution\"],\n",
        "            \"Department\": member[\"Department\"],\n",
        "            \"Academic Title\": member[\"Academic Title\"],\n",
        "            \"openalex_id\": author_data[\"author_id\"],\n",
        "            \"works_count\": author_data[\"works_count\"],\n",
        "            \"cited_by_count\": author_data[\"cited_by_count\"],\n",
        "        })\n",
        "    else:\n",
        "        results.append({\n",
        "            \"Name\": member[\"Name\"],\n",
        "            \"Institution\": member[\"Institution\"],\n",
        "            \"Department\": member[\"Department\"],\n",
        "            \"Academic Title\": member[\"Academic Title\"],\n",
        "            \"openalex_id\": None,\n",
        "            \"works_count\": None,\n",
        "            \"cited_by_count\": None,\n",
        "        })\n",
        "    # Respect API rate limits\n",
        "    time.sleep(1)  # Adjust delay if needed\n",
        "\n",
        "# Save the results to a JSON file\n",
        "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
        "    json.dump(results, file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Author IDs saved to '{output_file_path}'.\")\n",
        "\n",
        "# Optionally, save to CSV for easier inspection\n",
        "pd.DataFrame(results).to_csv(\"tuba_author_ids.csv\", index=False)\n",
        "print(\"Author IDs also saved to 'tuba_author_ids.csv'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcrwotOMEHrk",
        "outputId": "ffe6b451-1abe-442d-f664-1d9bbc2f3169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No OpenAlex ID for Atilla Abdulkadiroğlu. Skipping...\n",
            "No OpenAlex ID for M.talha Çiçek. Skipping...\n",
            "No OpenAlex ID for Prof. Dr. Robert Dankoff. Skipping...\n",
            "No OpenAlex ID for Ayşe Selçuk Esenbel. Skipping...\n",
            "No OpenAlex ID for K. Arzum  Erdem Gürsan. Skipping...\n",
            "No OpenAlex ID for Ahmet Saim Kılavuz. Skipping...\n",
            "No OpenAlex ID for Jeffry David Sachs. Skipping...\n",
            "H-index and I10-index added and saved to tuba_members_with_indices.json\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/tuba_author_ids.json\"  # Replace with your actual file path\n",
        "output_file = \"tuba_members_with_indices.json\"  # File to save the updated data\n",
        "\n",
        "# OpenAlex base API URL\n",
        "openalex_base_url = \"https://api.openalex.org\"\n",
        "\n",
        "# Function to fetch H-index and I10-index for an author\n",
        "def fetch_author_metrics(author_id):\n",
        "    url = f\"{openalex_base_url}/authors/{author_id}\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        h_index = data.get(\"summary_stats\", {}).get(\"h_index\", None)\n",
        "        i10_index = data.get(\"summary_stats\", {}).get(\"i10_index\", None)\n",
        "        return h_index, i10_index\n",
        "    else:\n",
        "        print(f\"Error fetching data for author {author_id}: {response.status_code}\")\n",
        "        return None, None\n",
        "\n",
        "# Load the existing JSON file\n",
        "with open(input_file, \"r\") as infile:\n",
        "    members = json.load(infile)\n",
        "\n",
        "# Process each member and fetch H-index and I10-index\n",
        "for member in members:\n",
        "    openalex_ids = member.get(\"openalex_id\")\n",
        "    if isinstance(openalex_ids, list):\n",
        "        # Initialize variables to aggregate metrics\n",
        "        h_indices = []\n",
        "        i10_indices = []\n",
        "        for openalex_id in openalex_ids:\n",
        "            author_id = openalex_id.split(\"/\")[-1]  # Extract author ID\n",
        "            h_index, i10_index = fetch_author_metrics(author_id)\n",
        "            if h_index is not None:\n",
        "                h_indices.append(h_index)\n",
        "            if i10_index is not None:\n",
        "                i10_indices.append(i10_index)\n",
        "            time.sleep(1)  # Respect API rate limits\n",
        "\n",
        "        # Aggregate indices (e.g., take maximum values)\n",
        "        member[\"h_index\"] = max(h_indices) if h_indices else None\n",
        "        member[\"i10_index\"] = max(i10_indices) if i10_indices else None\n",
        "\n",
        "    elif isinstance(openalex_ids, str):\n",
        "        author_id = openalex_ids.split(\"/\")[-1]  # Extract author ID\n",
        "        h_index, i10_index = fetch_author_metrics(author_id)\n",
        "        member[\"h_index\"] = h_index\n",
        "        member[\"i10_index\"] = i10_index\n",
        "        time.sleep(1)  # Respect API rate limits\n",
        "    else:\n",
        "        print(f\"No OpenAlex ID for {member['Name']}. Skipping...\")\n",
        "        member[\"h_index\"] = None\n",
        "        member[\"i10_index\"] = None\n",
        "\n",
        "# Save the updated JSON with indices\n",
        "with open(output_file, \"w\") as outfile:\n",
        "    json.dump(members, outfile, indent=4)\n",
        "\n",
        "print(f\"H-index and I10-index added and saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/tuba_members_with_indices.json\"  # Replace with your distorted JSON file path\n",
        "output_file = \"tuba_members_with_indices_restored.json\"  # Restored output file\n",
        "\n",
        "# Load the distorted JSON file\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "# Decode Unicode escape sequences\n",
        "def decode_unicode(data):\n",
        "    if isinstance(data, dict):\n",
        "        return {key: decode_unicode(value) for key, value in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [decode_unicode(item) for item in data]\n",
        "    elif isinstance(data, str):\n",
        "        try:\n",
        "            # Attempt to decode the string\n",
        "            return data.encode('latin1').decode('utf-8')\n",
        "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "            # Return the original string if decoding fails\n",
        "            return data\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "restored_data = decode_unicode(data)\n",
        "\n",
        "# Save the restored JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    json.dump(restored_data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Restored file saved to {output_file}\")"
      ],
      "metadata": {
        "id": "gEu4VmYpsvp_",
        "outputId": "a3af8ebf-6b35-4f08-ba6c-7703dfe90229",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored file saved to tuba_members_with_indices_restored.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/tuba_members_with_indices_restored.json\"  # Replace with your actual file path\n",
        "output_json_file = \"tuba_works_filtered.json\"  # JSON file output\n",
        "output_csv_file = \"tuba_works_filtered.csv\"  # CSV file output\n",
        "progress_log_file = \"progress_log4.json\"  # Log file to track progress\n",
        "\n",
        "# OpenAlex base API URL\n",
        "openalex_base_url = \"https://api.openalex.org\"\n",
        "\n",
        "# Function to fetch works for an author\n",
        "def fetch_author_works(author_id):\n",
        "    works = []\n",
        "    page = 1\n",
        "    while True:\n",
        "        url = f\"{openalex_base_url}/works?filter=author.id:{author_id}&per-page=200&page={page}\"\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            data = response.json()\n",
        "            for work in data.get(\"results\", []):  # Handle missing 'results'\n",
        "                filtered_work = {\n",
        "                    \"id\": work.get(\"id\", \"\"),\n",
        "                    \"title\": work.get(\"title\", \"\"),\n",
        "                    \"doi\": work.get(\"doi\", \"\"),\n",
        "                    \"publication_year\": work.get(\"publication_year\", \"\"),\n",
        "                    \"cited_by_count\": work.get(\"cited_by_count\", 0),\n",
        "                    \"authorships\": [\n",
        "                        {\n",
        "                            \"author\": {\n",
        "                                \"id\": auth.get(\"author\", {}).get(\"id\", \"\"),\n",
        "                                \"display_name\": auth.get(\"author\", {}).get(\"display_name\", \"\")\n",
        "                            },\n",
        "                            \"institutions\": [\n",
        "                                {\n",
        "                                    \"id\": inst.get(\"id\", \"\"),\n",
        "                                    \"display_name\": inst.get(\"display_name\", \"\"),\n",
        "                                    \"country_code\": inst.get(\"country_code\", \"\"),\n",
        "                                    \"type\": inst.get(\"type\", \"\")  # Safely handle missing type\n",
        "                                }\n",
        "                                for inst in auth.get(\"institutions\", []) if inst\n",
        "                            ]\n",
        "                        }\n",
        "                        for auth in work.get(\"authorships\", []) if auth\n",
        "                    ],\n",
        "                    \"primary_topic\": {\n",
        "                        \"id\": work.get(\"primary_topic\", {}).get(\"id\", \"\"),\n",
        "                        \"display_name\": work.get(\"primary_topic\", {}).get(\"display_name\", \"\"),\n",
        "                        \"score\": work.get(\"primary_topic\", {}).get(\"score\", 0),\n",
        "                        \"field\": {\n",
        "                            \"id\": work.get(\"primary_topic\", {}).get(\"field\", {}).get(\"id\", \"\"),\n",
        "                            \"display_name\": work.get(\"primary_topic\", {}).get(\"field\", {}).get(\"display_name\", \"\")\n",
        "                        } if work.get(\"primary_topic\", {}).get(\"field\") else {},\n",
        "                        \"subfield\": {\n",
        "                            \"id\": work.get(\"primary_topic\", {}).get(\"subfield\", {}).get(\"id\", \"\"),\n",
        "                            \"display_name\": work.get(\"primary_topic\", {}).get(\"subfield\", {}).get(\"display_name\", \"\")\n",
        "                        } if work.get(\"primary_topic\", {}).get(\"subfield\") else {},\n",
        "                        \"domain\": {\n",
        "                            \"id\": work.get(\"primary_topic\", {}).get(\"domain\", {}).get(\"id\", \"\"),\n",
        "                            \"display_name\": work.get(\"primary_topic\", {}).get(\"domain\", {}).get(\"display_name\", \"\")\n",
        "                        } if work.get(\"primary_topic\", {}).get(\"domain\") else {}\n",
        "                    } if work.get(\"primary_topic\") else {},  # Handle missing primary_topic\n",
        "                    \"concepts\": [\n",
        "                        {\n",
        "                            \"id\": concept.get(\"id\", \"\"),\n",
        "                            \"display_name\": concept.get(\"display_name\", \"\"),\n",
        "                            \"level\": concept.get(\"level\", 0),\n",
        "                            \"score\": concept.get(\"score\", 0)\n",
        "                        }\n",
        "                        for concept in work.get(\"concepts\", []) if concept\n",
        "                    ],\n",
        "                    \"open_access\": work.get(\"open_access\", {}),\n",
        "                    \"sustainable_development_goals\": [\n",
        "                        {\n",
        "                            \"id\": sdg.get(\"id\", \"\"),\n",
        "                            \"score\": sdg.get(\"score\", 0),\n",
        "                            \"display_name\": sdg.get(\"display_name\", \"\")\n",
        "                        }\n",
        "                        for sdg in work.get(\"sustainable_development_goals\", []) if sdg\n",
        "                    ],\n",
        "                    \"referenced_works\": work.get(\"referenced_works\", [])\n",
        "                }\n",
        "                works.append(filtered_work)\n",
        "            if \"next\" not in data.get(\"meta\", {}):  # Handle missing 'meta'\n",
        "                break\n",
        "            page += 1\n",
        "            time.sleep(1)  # Respect API rate limits\n",
        "        else:\n",
        "            print(f\"Error fetching works for author {author_id}: {response.status_code}\")\n",
        "            break\n",
        "    return works\n",
        "\n",
        "# Load the existing JSON file\n",
        "with open(input_file, \"r\") as infile:\n",
        "    members = json.load(infile)\n",
        "\n",
        "# Load or initialize the progress log\n",
        "try:\n",
        "    with open(progress_log_file, \"r\") as log_file:\n",
        "        progress_log = json.load(log_file)\n",
        "except FileNotFoundError:\n",
        "    progress_log = {}\n",
        "\n",
        "# Fetch works for each author and organize them under authors\n",
        "authors_with_works = []\n",
        "for member in members:\n",
        "    name = member[\"Name\"]\n",
        "    openalex_ids = member.get(\"openalex_id\")\n",
        "    all_works = []\n",
        "\n",
        "    if name in progress_log and progress_log[name]:\n",
        "        print(f\"Skipping {name}, already completed.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"Fetching works for {name}...\")\n",
        "\n",
        "    try:\n",
        "        if isinstance(openalex_ids, list):\n",
        "            for openalex_id in openalex_ids:\n",
        "                author_id = openalex_id.split(\"/\")[-1]  # Extract author ID\n",
        "                all_works.extend(fetch_author_works(author_id))\n",
        "        elif isinstance(openalex_ids, str):\n",
        "            author_id = openalex_ids.split(\"/\")[-1]\n",
        "            all_works.extend(fetch_author_works(author_id))\n",
        "\n",
        "        authors_with_works.append({\n",
        "            \"name\": name,\n",
        "            \"openalex_id\": openalex_ids,\n",
        "            \"works\": all_works\n",
        "        })\n",
        "\n",
        "        # Mark the author as successfully processed\n",
        "        progress_log[name] = True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {name}: {e}\")\n",
        "        progress_log[name] = False  # Mark as failed\n",
        "\n",
        "    # Save progress after each author\n",
        "    with open(progress_log_file, \"w\") as log_file:\n",
        "        json.dump(progress_log, log_file, indent=4)\n",
        "\n",
        "# Save the filtered data to a JSON file\n",
        "with open(output_json_file, \"w\") as outfile:\n",
        "    json.dump(authors_with_works, outfile, indent=4)\n",
        "print(f\"Filtered works data saved to JSON: {output_json_file}\")\n",
        "\n",
        "# Convert to CSV for easier inspection\n",
        "csv_data = []\n",
        "for author in authors_with_works:\n",
        "    for work in author[\"works\"]:\n",
        "        csv_data.append({\n",
        "            \"author_name\": author[\"name\"],\n",
        "            \"author_openalex_id\": author[\"openalex_id\"],\n",
        "            \"work_id\": work[\"id\"],\n",
        "            \"work_title\": work[\"title\"],\n",
        "            \"doi\": work[\"doi\"],\n",
        "            \"publication_year\": work[\"publication_year\"],\n",
        "            \"cited_by_count\": work[\"cited_by_count\"],\n",
        "            \"field\": work[\"primary_topic\"].get(\"field\", {}).get(\"display_name\", \"\"),\n",
        "            \"subfield\": work[\"primary_topic\"].get(\"subfield\", {}).get(\"display_name\", \"\"),\n",
        "            \"domain\": work[\"primary_topic\"].get(\"domain\", {}).get(\"display_name\", \"\"),\n",
        "            \"concepts\": \", \".join([concept[\"display_name\"] for concept in work[\"concepts\"]]),\n",
        "            \"open_access_status\": work.get(\"open_access\", {}).get(\"oa_status\", \"\"),\n",
        "            \"sustainable_development_goals\": \", \".join([sdg[\"display_name\"] for sdg in work.get(\"sustainable_development_goals\", [])]),\n",
        "            \"referenced_works_count\": len(work.get(\"referenced_works\", []))\n",
        "        })\n",
        "\n",
        "pd.DataFrame(csv_data).to_csv(output_csv_file, index=False)\n",
        "print(f\"Filtered works data saved to CSV: {output_csv_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU83d0m78eUT",
        "outputId": "1018b3b5-38ea-4ae8-9877-e56cd503d5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching works for Muzaffer Şeker...\n",
            "Fetching works for Atilla Abdulkadiroğlu...\n",
            "Fetching works for Mustafa Acar...\n",
            "Fetching works for Ahmet Cevat Acar...\n",
            "Fetching works for Alparslan Açıkgenç...\n",
            "Fetching works for Nazmi Volkan Adsay...\n",
            "Fetching works for Ali Ekber Akgün...\n",
            "Fetching works for M. İrşadi Aksun...\n",
            "Fetching works for Yasin Aktay...\n",
            "Fetching works for Şener Aktürk...\n",
            "Fetching works for Ali Akyıldız...\n",
            "Fetching works for Mehmet Hakkı Alma...\n",
            "Fetching works for Meliha Altunışık...\n",
            "Fetching works for M. Fatih Andı...\n",
            "Fetching works for Mustafa Reşat Apak...\n",
            "Fetching works for Erol Arcaklıoğlu...\n",
            "Fetching works for Metin Arık...\n",
            "Fetching works for Erdal Arıkan...\n",
            "Fetching works for Hüseyin Arslan...\n",
            "Fetching works for Ercümend Arvas...\n",
            "Fetching works for Messoud Ashina...\n",
            "Fetching works for Abdullah Atalar...\n",
            "Fetching works for Ali Tayfun Atay...\n",
            "Fetching works for Abdurrahman Atçıl...\n",
            "Fetching works for Cenk Ayata...\n",
            "Fetching works for Orhan Aydın...\n",
            "Fetching works for Mehmet Emin Aydın...\n",
            "Fetching works for Ahmet Faruk Aysan...\n",
            "Fetching works for Mehmet Baç...\n",
            "Fetching works for Ali Müfit Bahadır...\n",
            "Fetching works for Sezgin Bakırdere...\n",
            "Fetching works for Ali Balcı...\n",
            "Fetching works for Metin Balcı...\n",
            "Fetching works for Erden Banoğlu...\n",
            "Fetching works for Ertuğrul Başar...\n",
            "Fetching works for Yıldız Bayazıtoğlu...\n",
            "Fetching works for Adrian Bejan...\n",
            "Fetching works for Özer  Bekaroğlu...\n",
            "Fetching works for Meral Beksaç...\n",
            "Fetching works for Hayrunnisa Bolay Belen...\n",
            "Fetching works for A. Nihat Berker...\n",
            "Fetching works for Zulfiqar Ahmed Bhutta...\n",
            "Fetching works for Pınar Bilgin...\n",
            "Fetching works for Erhan Bişkin...\n",
            "Fetching works for Mehmet Bulut...\n",
            "Fetching works for Ömer Çaha...\n",
            "Fetching works for Ömer Ziya Cebeci...\n",
            "Fetching works for Mehmet Çelik...\n",
            "Fetching works for Gülfettin Çelik...\n",
            "Fetching works for Bekir Çetinkaya...\n",
            "Fetching works for Abdulkadir Çevik...\n",
            "Fetching works for M.talha Çiçek...\n",
            "Fetching works for Mustafa Çiçekler...\n",
            "Fetching works for Ümit Cizre...\n",
            "Fetching works for Salim Çıracı...\n",
            "Fetching works for Amnon Cohen...\n",
            "Fetching works for Turgay Dalkara...\n",
            "Fetching works for Prof. Dr. Robert Dankoff...\n",
            "Fetching works for Hilmi Volkan Demir...\n",
            "Fetching works for Şeref Demirayak...\n",
            "Fetching works for Taner Demirer...\n",
            "Fetching works for Bilge Demirköz...\n",
            "Fetching works for Adil Denizli...\n",
            "Fetching works for Tekin Dereli...\n",
            "Fetching works for Stefan Dimitrov...\n",
            "Fetching works for İbrahim Dinçer...\n",
            "Fetching works for Seydi Doğan...\n",
            "Fetching works for Timur Doğu...\n",
            "Fetching works for Oktay Duman...\n",
            "Fetching works for Engin Durgun...\n",
            "Fetching works for Nazım Ekren...\n",
            "Fetching works for Muzaffer Elmas...\n",
            "Fetching works for Feridun M. Emecen...\n",
            "Fetching works for Sevim Ercan...\n",
            "Fetching works for Bayram Zafer Erdoğan...\n",
            "Fetching works for Mustafa Erdoğan...\n",
            "Fetching works for Özcan Erel...\n",
            "Fetching works for Mustafa Evren Erşahin...\n",
            "Fetching works for Mustafa Ersöz...\n",
            "Fetching works for Ayşe Selçuk Esenbel...\n",
            "Fetching works for Zafer Evis...\n",
            "Fetching works for Carter Vaughn Findley...\n",
            "Fetching works for Tülin Gençöz...\n",
            "Fetching works for Vural Gökmen...\n",
            "Fetching works for Nilüfer Göle...\n",
            "Fetching works for Ahmet Gül...\n",
            "Fetching works for Aydın Gülan...\n",
            "Fetching works for İlhami Gülçin...\n",
            "Fetching works for Fatih Gültekin...\n",
            "Fetching works for Naci Gündoğan...\n",
            "Fetching works for Şinasi Gündüz...\n",
            "Fetching works for Mesut Güner...\n",
            "Fetching works for Bahar Güntekin...\n",
            "Fetching works for K. Arzum  Erdem Gürsan...\n",
            "Fetching works for Esra Çapanoğlu Güven...\n",
            "Fetching works for Wael B. Hallaq...\n",
            "Fetching works for Gabor Hamza...\n",
            "Fetching works for M. Şükrü Hanioğlu...\n",
            "Fetching works for İbrahim Hatiboğlu...\n",
            "Fetching works for Ali Adnan Hayaloğlu...\n",
            "Fetching works for Kadir Mutlu Hayran...\n",
            "Fetching works for Arif Hepbaşlı...\n",
            "Fetching works for Metin Heper...\n",
            "Fetching works for Gökhan Hotamışlıgil...\n",
            "Fetching works for Orhan İçelli...\n",
            "Fetching works for Ekmeleddin İhsanoğlu...\n",
            "Fetching works for Umran Savaş İnan...\n",
            "Fetching works for Erik Jeppesen...\n",
            "Fetching works for Shuanggen Jin...\n",
            "Fetching works for Mustafa S. Kaçalin...\n",
            "Fetching works for Cemal Kafadar...\n",
            "Fetching works for Esin Kahya...\n",
            "Fetching works for Sadık Kakaç...\n",
            "Fetching works for Münci Kalayoğlu...\n",
            "Fetching works for Mahmut Kandemir...\n",
            "Fetching works for Emin Kansu...\n",
            "Fetching works for Muhsin Kar...\n",
            "Fetching works for İsmail Kara...\n",
            "Fetching works for Ramazan Karakuzu...\n",
            "Fetching works for Tanju Karanfil...\n",
            "Fetching works for Taşkın Kavzoğlu...\n",
            "Fetching works for Kamil Kaygusuz...\n",
            "Fetching works for Okyay Kaynak...\n",
            "Fetching works for Ali Keleş...\n",
            "Fetching works for Hasan Fahrettin Keleştemur...\n",
            "Fetching works for Halit Keskin...\n",
            "Fetching works for Mary-Claire King...\n",
            "Fetching works for Alper Kiraz...\n",
            "Fetching works for Özgür Kişi...\n",
            "Fetching works for Ahmet Saim Kılavuz...\n",
            "Fetching works for Ertuğrul Kılıç...\n",
            "Fetching works for Zeynel Kılıç...\n",
            "Fetching works for Atıf Koca...\n",
            "Fetching works for Mustafa Verşan Kök...\n",
            "Fetching works for Ayşegül Komsuoğlu...\n",
            "Fetching works for Feza Korkusuz...\n",
            "Fetching works for Ali Koşar...\n",
            "Fetching works for İsmail Koyuncu...\n",
            "Fetching works for Ömer Küçük...\n",
            "Fetching works for Filiz Kuralay...\n",
            "Fetching works for Hamza Kurt...\n",
            "Fetching works for İlhan Kutluer...\n",
            "Fetching works for Bold Luvsandorj...\n",
            "Fetching works for Hasan Mandal...\n",
            "Fetching works for Önder Metin...\n",
            "Fetching works for Ali Mostafazadeh...\n",
            "Fetching works for Şaban Nazlıoğlu...\n",
            "Fetching works for Ahmet Yaşar Ocak...\n",
            "Fetching works for Oğuz Okay...\n",
            "Fetching works for Ziya Öniş...\n",
            "Fetching works for İlkay Erdoğan Orhan...\n",
            "Fetching works for Necati Örmeci...\n",
            "Fetching works for İlber Ortaylı...\n",
            "Fetching works for Mahmut Özacar...\n",
            "Fetching works for Haldun M. Özaktaş...\n",
            "Fetching works for Ekmel Özbay...\n",
            "Fetching works for Yusuf Ziya Özcan...\n",
            "Fetching works for Hasan Tayfun Özçelik...\n",
            "Fetching works for Ali Özer...\n",
            "Fetching works for İzzet Özgenç...\n",
            "Fetching works for Cengiz Sinan Özkan...\n",
            "Fetching works for Mehmed Özkan...\n",
            "Fetching works for Saim Özkar...\n",
            "Fetching works for Ali Rıza Özkaya...\n",
            "Fetching works for Mehmet Öztürk...\n",
            "Fetching works for Özcan Öztürk...\n",
            "Fetching works for İzzet Öztürk...\n",
            "Fetching works for Mehmet Akif Öztürk...\n",
            "Fetching works for Mustafa Serdar Palabıyık...\n",
            "Fetching works for Viorel Panaite...\n",
            "Fetching works for Hakan Parlakpınar...\n",
            "Fetching works for Erol Pehlivan...\n",
            "Fetching works for Necati Polat...\n",
            "Fetching works for Jamil Ragep...\n",
            "Fetching works for Jeffry David Sachs...\n",
            "Fetching works for Mustafa Safran...\n",
            "Fetching works for Bahri Şahin...\n",
            "Fetching works for Kazım Şahin...\n",
            "Fetching works for Fikrettin Şahin...\n",
            "Fetching works for Mustafa Şahmaran...\n",
            "Fetching works for Bekir Salih...\n",
            "Fetching works for Aziz Sancar...\n",
            "Fetching works for M. A. Yekta Saraç...\n",
            "Fetching works for Niyazi Serdar Sarıçiftçi...\n",
            "Fetching works for Hüseyin Sarıoğlu...\n",
            "Fetching works for Ekrem Savaş...\n",
            "Fetching works for Mustafa Solak...\n",
            "Fetching works for Mustafa Soylak...\n",
            "Fetching works for Yunus Söylet...\n",
            "Fetching works for Joseph Jao-Yiu Sung...\n",
            "Fetching works for Ahmet Tabakoğlu...\n",
            "Fetching works for Candan Tamerler...\n",
            "Fetching works for Nusret Tan...\n",
            "Fetching works for Bilal Tanatar...\n",
            "Fetching works for Feridun Cahit Tanyel...\n",
            "Fetching works for Ekrem Tatoğlu...\n",
            "Fetching works for Ahmet Murat Tekalp...\n",
            "Fetching works for İsenbike Togan...\n",
            "Fetching works for Haluk Aydın Topaloğlu...\n",
            "Fetching works for S. Ali Tuncel...\n",
            "Fetching works for Mustafa Tüzen...\n",
            "Fetching works for Saffet Tüzgen...\n",
            "Fetching works for Mehmet Fatih Uçkun...\n",
            "Fetching works for İsmail Hakkı Ulus...\n",
            "Fetching works for Joseph Wang...\n",
            "Fetching works for Omar Yaghi...\n",
            "Fetching works for M. Cemal Yalabık...\n",
            "Fetching works for Halit Yanıkkaya...\n",
            "Fetching works for Gazi Yaşargil...\n",
            "Fetching works for Serap Yazıcı...\n",
            "Fetching works for Okan Zafer Yeşilel...\n",
            "Fetching works for Jackie Y. Ying...\n",
            "Fetching works for Hüseyin Yıldırım...\n",
            "Fetching works for Ali Rıza Yıldız...\n",
            "Fetching works for Mehmet Emin Yılmaz...\n",
            "Fetching works for Mustafa Yücel...\n",
            "Fetching works for Ahmet Nuri Yurdusev...\n",
            "Fetching works for Mehmet Zahmakıran...\n",
            "Fetching works for Erkan Zergeroğlu...\n",
            "Fetching works for Bülent Zülfikar...\n",
            "Filtered works data saved to JSON: tuba_works_filtered.json\n",
            "Filtered works data saved to CSV: tuba_works_filtered.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/tuba_works_filtered.json\"  # Replace with your distorted JSON file path\n",
        "output_file = \"tuba_works_filtered_restored.json\"  # Restored output file\n",
        "\n",
        "# Load the distorted JSON file\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as infile:\n",
        "    data = json.load(infile)\n",
        "\n",
        "# Decode Unicode escape sequences\n",
        "def decode_unicode(data):\n",
        "    if isinstance(data, dict):\n",
        "        return {key: decode_unicode(value) for key, value in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [decode_unicode(item) for item in data]\n",
        "    elif isinstance(data, str):\n",
        "        try:\n",
        "            # Attempt to decode the string\n",
        "            return data.encode('latin1').decode('utf-8')\n",
        "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "            # Return the original string if decoding fails\n",
        "            return data\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "restored_data = decode_unicode(data)\n",
        "\n",
        "# Save the restored JSON file\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    json.dump(restored_data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Restored file saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yQy963dKTDn",
        "outputId": "3aa88590-ee95-4ed2-f9e0-06e19a079153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored file saved to tuba_works_filtered_restored.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Input and output file paths\n",
        "input_file = \"/content/tuba_works_filtered.csv\"  # Replace with your distorted CSV file path\n",
        "output_file = \"tuba_works_filtered_restored.csv\"  # Restored output file\n",
        "\n",
        "# Load the distorted CSV file\n",
        "df = pd.read_csv(input_file)\n",
        "\n",
        "# Decode Unicode escape sequences in strings\n",
        "def decode_unicode_string(value):\n",
        "    if isinstance(value, str):  # Ensure the value is a string before decoding\n",
        "        try:\n",
        "            return value.encode('latin1').decode('utf-8')\n",
        "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
        "            return value  # Return the original value if decoding fails\n",
        "    return value  # Return the value as-is if it's not a string\n",
        "\n",
        "# Apply the Unicode decoding function to all string cells in the DataFrame\n",
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':  # Apply only to object (string) columns\n",
        "        df[col] = df[col].apply(decode_unicode_string)\n",
        "\n",
        "# Save the restored CSV file\n",
        "df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Restored file saved to {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EXO6sk9LYpN",
        "outputId": "3a0978a4-992a-40e2-cd8d-bf0dcc07c99f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Restored file saved to tuba_works_filtered_restored.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}